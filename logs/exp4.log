Training starting...
Datetime: 2023-04-28 18:14:20.663602
wandb: Currently logged in as: oriolmartinez (tfg_oriol). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /home-net/omartinez/TFG/wandb/run-20230428_181423-x4upsm52
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-river-28
wandb: ⭐️ View project at https://wandb.ai/tfg_oriol/action_classification
wandb: 🚀 View run at https://wandb.ai/tfg_oriol/action_classification/runs/x4upsm52
NAME: action_classification
dataset:
  NAME: epic_kitchens
  FRAME_SIZE: 112
  NUM_FRAMES: 100
  IN_CHANNELS: 3
model:
  ATTENTION: vanilla_attention
  NUM_CLASSES: 96
  PATCH_SIZE: 16
  DEPTH: 2
  HEADS: 4
training:
  EPOCHS: 1
  SEED: 0
  BATCH_SIZE: 2
  DATA_THREADS: 5
  PRINT_BATCH: 100
  LEARNING_RATE: 0.0001
  PRETRAINED_STATE_PATH: None
  GPU: 5
inference:
  WEIGHTS_PATH: /home-net/omartinez/TFG/weights/
  MODEL: vanilla_attention_1

Loading the data...
Loading the training dataset
Loading the validation dataset
Epoch 0/0
----------
../aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
 - Batch Number 100 -> Loss: 2.229 Accuracy: 0.055
 - Batch Number 200 -> Loss: 0.980 Accuracy: 0.058
 - Batch Number 300 -> Loss: 0.615 Accuracy: 0.057
 - Batch Number 400 -> Loss: 0.488 Accuracy: 0.049
 - Batch Number 500 -> Loss: 0.363 Accuracy: 0.050
 - Batch Number 600 -> Loss: 0.326 Accuracy: 0.047
 - Batch Number 700 -> Loss: 0.250 Accuracy: 0.044
 - Batch Number 800 -> Loss: 0.210 Accuracy: 0.043
 - Batch Number 900 -> Loss: 0.207 Accuracy: 0.042
 - Batch Number 1000 -> Loss: 0.179 Accuracy: 0.042
 - Batch Number 1100 -> Loss: 0.140 Accuracy: 0.041
 - Batch Number 1200 -> Loss: 0.150 Accuracy: 0.040
 - Batch Number 1300 -> Loss: 0.138 Accuracy: 0.040
 - Batch Number 1400 -> Loss: 0.124 Accuracy: 0.043
 - Batch Number 1500 -> Loss: 0.119 Accuracy: 0.043
 - Batch Number 1600 -> Loss: 0.111 Accuracy: 0.045
 - Batch Number 1700 -> Loss: 0.102 Accuracy: 0.044
 - Batch Number 1800 -> Loss: 0.102 Accuracy: 0.043
 - Batch Number 1900 -> Loss: 0.092 Accuracy: 0.042
 - Batch Number 2000 -> Loss: 0.086 Accuracy: 0.041
 - Batch Number 2100 -> Loss: 0.078 Accuracy: 0.040
 - Batch Number 2200 -> Loss: 0.077 Accuracy: 0.040
 - Batch Number 2300 -> Loss: 0.077 Accuracy: 0.039
 - Batch Number 2400 -> Loss: 0.074 Accuracy: 0.038
 - Batch Number 2500 -> Loss: 0.067 Accuracy: 0.038
 - Batch Number 2600 -> Loss: 0.073 Accuracy: 0.037
 - Batch Number 2700 -> Loss: 0.065 Accuracy: 0.037
 - Batch Number 2800 -> Loss: 0.063 Accuracy: 0.036
 - Batch Number 2900 -> Loss: 0.060 Accuracy: 0.036
 - Batch Number 3000 -> Loss: 0.059 Accuracy: 0.034
 - Batch Number 3100 -> Loss: 0.059 Accuracy: 0.034
 - Batch Number 3200 -> Loss: 0.056 Accuracy: 0.034
 - Batch Number 3300 -> Loss: 0.053 Accuracy: 0.033
 - Batch Number 3400 -> Loss: 0.054 Accuracy: 0.033
 - Batch Number 3500 -> Loss: 0.053 Accuracy: 0.033
 - Batch Number 3600 -> Loss: 0.047 Accuracy: 0.033
 - Batch Number 3700 -> Loss: 0.051 Accuracy: 0.033
 - Batch Number 3800 -> Loss: 0.050 Accuracy: 0.033
 - Batch Number 3900 -> Loss: 0.042 Accuracy: 0.033
 - Batch Number 4000 -> Loss: 0.040 Accuracy: 0.033
 - Batch Number 4100 -> Loss: 0.044 Accuracy: 0.034
 - Batch Number 4200 -> Loss: 0.044 Accuracy: 0.033
 - Batch Number 4300 -> Loss: 0.042 Accuracy: 0.033
 - Batch Number 4400 -> Loss: 0.039 Accuracy: 0.033
 - Batch Number 4500 -> Loss: 0.038 Accuracy: 0.032
 - Batch Number 4600 -> Loss: 0.037 Accuracy: 0.032
 - Batch Number 4700 -> Loss: 0.037 Accuracy: 0.033
 - Batch Number 4800 -> Loss: 0.034 Accuracy: 0.033
 - Batch Number 4900 -> Loss: 0.035 Accuracy: 0.033
 - Batch Number 5000 -> Loss: 0.034 Accuracy: 0.033
 - Batch Number 5100 -> Loss: 0.031 Accuracy: 0.033
 - Batch Number 5200 -> Loss: 0.035 Accuracy: 0.033
 - Batch Number 5300 -> Loss: 0.031 Accuracy: 0.032
 - Batch Number 5400 -> Loss: 0.032 Accuracy: 0.032
 - Batch Number 5500 -> Loss: 0.030 Accuracy: 0.032
 - Batch Number 5600 -> Loss: 0.033 Accuracy: 0.031
 - Batch Number 5700 -> Loss: 0.030 Accuracy: 0.031
 - Batch Number 5800 -> Loss: 0.030 Accuracy: 0.031
 - Batch Number 5900 -> Loss: 0.029 Accuracy: 0.031
 - Batch Number 6000 -> Loss: 0.030 Accuracy: 0.030
 - Batch Number 6100 -> Loss: 0.028 Accuracy: 0.030
 - Batch Number 6200 -> Loss: 0.027 Accuracy: 0.030
 - Batch Number 6300 -> Loss: 0.027 Accuracy: 0.030
 - Batch Number 6400 -> Loss: 0.026 Accuracy: 0.030
 - Batch Number 6500 -> Loss: 0.024 Accuracy: 0.030
 - Batch Number 6600 -> Loss: 0.027 Accuracy: 0.030
 - Batch Number 6700 -> Loss: 0.027 Accuracy: 0.030
 - Batch Number 6800 -> Loss: 0.025 Accuracy: 0.030
 - Batch Number 6900 -> Loss: 0.024 Accuracy: 0.030
 - Batch Number 7000 -> Loss: 0.024 Accuracy: 0.030
 - Batch Number 7100 -> Loss: 0.023 Accuracy: 0.030
 - Batch Number 7200 -> Loss: 0.024 Accuracy: 0.030
 - Batch Number 7300 -> Loss: 0.024 Accuracy: 0.030
 - Batch Number 7400 -> Loss: 0.023 Accuracy: 0.030
 - Batch Number 7500 -> Loss: 0.025 Accuracy: 0.030
 - Batch Number 7600 -> Loss: 0.023 Accuracy: 0.030
 - Batch Number 7700 -> Loss: 0.022 Accuracy: 0.030
 - Batch Number 7800 -> Loss: 0.021 Accuracy: 0.030
 - Batch Number 7900 -> Loss: 0.022 Accuracy: 0.030
 - Batch Number 8000 -> Loss: 0.021 Accuracy: 0.030
 - Batch Number 8100 -> Loss: 0.023 Accuracy: 0.030
 - Batch Number 8200 -> Loss: 0.021 Accuracy: 0.030
 - Batch Number 8300 -> Loss: 0.019 Accuracy: 0.030
 - Batch Number 8400 -> Loss: 0.021 Accuracy: 0.030
 - Batch Number 8500 -> Loss: 0.021 Accuracy: 0.030
 - Batch Number 8600 -> Loss: 0.020 Accuracy: 0.029
 - Batch Number 8700 -> Loss: 0.020 Accuracy: 0.029
 - Batch Number 8800 -> Loss: 0.018 Accuracy: 0.029
 - Batch Number 8900 -> Loss: 0.019 Accuracy: 0.029
 - Batch Number 9000 -> Loss: 0.019 Accuracy: 0.029
 - Batch Number 9100 -> Loss: 0.020 Accuracy: 0.029
 - Batch Number 9200 -> Loss: 0.019 Accuracy: 0.029
 - Batch Number 9300 -> Loss: 0.018 Accuracy: 0.029
 - Batch Number 9400 -> Loss: 0.019 Accuracy: 0.029
 - Batch Number 9500 -> Loss: 0.019 Accuracy: 0.029
 - Batch Number 9600 -> Loss: 0.017 Accuracy: 0.029
 - Batch Number 9700 -> Loss: 0.017 Accuracy: 0.029
 - Batch Number 9800 -> Loss: 0.018 Accuracy: 0.029
 - Batch Number 9900 -> Loss: 0.016 Accuracy: 0.028
 - Batch Number 10000 -> Loss: 0.019 Accuracy: 0.028
 - Batch Number 10100 -> Loss: 0.017 Accuracy: 0.028
 - Batch Number 10200 -> Loss: 0.018 Accuracy: 0.028
 - Batch Number 10300 -> Loss: 0.016 Accuracy: 0.028
 - Batch Number 10400 -> Loss: 0.016 Accuracy: 0.028
 - Batch Number 10500 -> Loss: 0.016 Accuracy: 0.028
 - Batch Number 10600 -> Loss: 0.015 Accuracy: 0.028
 - Batch Number 10700 -> Loss: 0.015 Accuracy: 0.028
 - Batch Number 10800 -> Loss: 0.016 Accuracy: 0.028
 - Batch Number 10900 -> Loss: 0.017 Accuracy: 0.028
 - Batch Number 11000 -> Loss: 0.015 Accuracy: 0.028
 - Batch Number 11100 -> Loss: 0.015 Accuracy: 0.028
 - Batch Number 11200 -> Loss: 0.015 Accuracy: 0.028
 - Batch Number 11300 -> Loss: 0.015 Accuracy: 0.028
 - Batch Number 11400 -> Loss: 0.014 Accuracy: 0.027
 - Batch Number 11500 -> Loss: 0.014 Accuracy: 0.027
 - Batch Number 11600 -> Loss: 0.015 Accuracy: 0.028
 - Batch Number 11700 -> Loss: 0.015 Accuracy: 0.028
 - Batch Number 11800 -> Loss: 0.015 Accuracy: 0.028
 - Batch Number 11900 -> Loss: 0.013 Accuracy: 0.027
 - Batch Number 12000 -> Loss: 0.015 Accuracy: 0.027
 - Batch Number 12100 -> Loss: 0.014 Accuracy: 0.027
 - Batch Number 12200 -> Loss: 0.013 Accuracy: 0.027
 - Batch Number 12300 -> Loss: 0.013 Accuracy: 0.027
 - Batch Number 12400 -> Loss: 0.013 Accuracy: 0.027
 - Batch Number 12500 -> Loss: 0.013 Accuracy: 0.027
 - Batch Number 12600 -> Loss: 0.014 Accuracy: 0.027
 - Batch Number 12700 -> Loss: 0.013 Accuracy: 0.027
 - Batch Number 12800 -> Loss: 0.014 Accuracy: 0.027
 - Batch Number 12900 -> Loss: 0.013 Accuracy: 0.027
 - Batch Number 13000 -> Loss: 0.012 Accuracy: 0.027
 - Batch Number 13100 -> Loss: 0.012 Accuracy: 0.027
 - Batch Number 13200 -> Loss: 0.013 Accuracy: 0.027
 - Batch Number 13300 -> Loss: 0.013 Accuracy: 0.027
 - Batch Number 13400 -> Loss: 0.013 Accuracy: 0.027
 - Batch Number 13500 -> Loss: 0.012 Accuracy: 0.027
 - Batch Number 13600 -> Loss: 0.013 Accuracy: 0.027
 - Batch Number 13700 -> Loss: 0.012 Accuracy: 0.027
 - Batch Number 13800 -> Loss: 0.012 Accuracy: 0.027
 - Batch Number 13900 -> Loss: 0.012 Accuracy: 0.027
Error executing job with overrides: ['training.GPU=5']
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: \ 0.006 MB of 0.018 MB uploaded (0.000 MB deduped)wandb: | 0.006 MB of 0.018 MB uploaded (0.000 MB deduped)wandb: / 0.018 MB of 0.018 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:      train_batches/batch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  train_batches/train_acc █▆▅▄▅▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb: train_batches/train_loss █▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_batches/batch 13900
wandb:  train_batches/train_acc 0.02683
wandb: train_batches/train_loss 0.01174
wandb: 
wandb: 🚀 View run playful-river-28 at: https://wandb.ai/tfg_oriol/action_classification/runs/x4upsm52
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230428_181423-x4upsm52/logs
Traceback (most recent call last):
  File "main.py", line 148, in <module>
    run_experiment()
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "main.py", line 126, in run_experiment
    trained_model = train_model(model, dataloaders, criterion, optimizer, DEVICE, num_epochs, print_batch)
  File "/home-net/omartinez/TFG/train.py", line 58, in train_model
    scaler.step(optimizer)
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 370, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 289, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py", line 289, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


Training starting...
Datetime: 2023-05-04 11:04:15.976918
wandb: Currently logged in as: oriolmartinez (tfg_oriol). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /home-net/omartinez/TFG/wandb/run-20230504_110419-31k69y5g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run holographic-droid-49
wandb: ⭐️ View project at https://wandb.ai/tfg_oriol/action_classification
wandb: 🚀 View run at https://wandb.ai/tfg_oriol/action_classification/runs/31k69y5g
NAME: action_classification
dataset:
  NAME: epic_kitchens
  FRAME_SIZE: 112
  NUM_FRAMES: 100
  IN_CHANNELS: 3
model:
  ATTENTION: skyformer
  accumulation: 1
  num_feats: 128
  NUM_CLASSES: 96
  PATCH_SIZE: 16
  DEPTH: 2
  HEADS: 4
training:
  EPOCHS: 4
  SEED: 0
  BATCH_SIZE: 2
  DATA_THREADS: 5
  PRINT_BATCH: 100
  LEARNING_RATE: 1.0e-06
  PRETRAINED_STATE_PATH: None
  GPU: 2
inference:
  WEIGHTS_PATH: /home-net/omartinez/TFG/weights/
  MODEL: vanilla_attention_1

Loading the data...
Loading the training dataset
Loading the validation dataset
Epoch 0/3
----------
 - Batch Number 100 -> Loss: 2.284 Accuracy: 0.020
 - Batch Number 200 -> Loss: 1.141 Accuracy: 0.020
 - Batch Number 300 -> Loss: 0.760 Accuracy: 0.018
 - Batch Number 400 -> Loss: 0.570 Accuracy: 0.016
 - Batch Number 500 -> Loss: 0.456 Accuracy: 0.019
 - Batch Number 600 -> Loss: 0.380 Accuracy: 0.021
 - Batch Number 700 -> Loss: 0.325 Accuracy: 0.022
 - Batch Number 800 -> Loss: 0.285 Accuracy: 0.025
 - Batch Number 900 -> Loss: 0.254 Accuracy: 0.025
 - Batch Number 1000 -> Loss: 0.228 Accuracy: 0.025
 - Batch Number 1100 -> Loss: 0.208 Accuracy: 0.026
 - Batch Number 1200 -> Loss: 0.190 Accuracy: 0.026
 - Batch Number 1300 -> Loss: 0.175 Accuracy: 0.027
 - Batch Number 1400 -> Loss: 0.163 Accuracy: 0.030
 - Batch Number 1500 -> Loss: 0.152 Accuracy: 0.032
 - Batch Number 1600 -> Loss: 0.142 Accuracy: 0.033
 - Batch Number 1700 -> Loss: 0.134 Accuracy: 0.034
 - Batch Number 1800 -> Loss: 0.126 Accuracy: 0.035
 - Batch Number 1900 -> Loss: 0.120 Accuracy: 0.037
 - Batch Number 2000 -> Loss: 0.114 Accuracy: 0.036
 - Batch Number 2100 -> Loss: 0.108 Accuracy: 0.038
 - Batch Number 2200 -> Loss: 0.104 Accuracy: 0.038
 - Batch Number 2300 -> Loss: 0.099 Accuracy: 0.039
 - Batch Number 2400 -> Loss: 0.095 Accuracy: 0.040
 - Batch Number 2500 -> Loss: 0.091 Accuracy: 0.040
 - Batch Number 2600 -> Loss: 0.088 Accuracy: 0.040
 - Batch Number 2700 -> Loss: 0.084 Accuracy: 0.040
 - Batch Number 2800 -> Loss: 0.081 Accuracy: 0.041
 - Batch Number 2900 -> Loss: 0.078 Accuracy: 0.041
 - Batch Number 3000 -> Loss: 0.076 Accuracy: 0.042
 - Batch Number 3100 -> Loss: 0.073 Accuracy: 0.043
 - Batch Number 3200 -> Loss: 0.071 Accuracy: 0.043
 - Batch Number 3300 -> Loss: 0.069 Accuracy: 0.045
 - Batch Number 3400 -> Loss: 0.067 Accuracy: 0.046
 - Batch Number 3500 -> Loss: 0.065 Accuracy: 0.047
 - Batch Number 3600 -> Loss: 0.063 Accuracy: 0.047
 - Batch Number 3700 -> Loss: 0.061 Accuracy: 0.048
 - Batch Number 3800 -> Loss: 0.060 Accuracy: 0.049
 - Batch Number 3900 -> Loss: 0.058 Accuracy: 0.050
 - Batch Number 4000 -> Loss: 0.057 Accuracy: 0.050
 - Batch Number 4100 -> Loss: 0.055 Accuracy: 0.050
 - Batch Number 4200 -> Loss: 0.054 Accuracy: 0.051
 - Batch Number 4300 -> Loss: 0.053 Accuracy: 0.052
 - Batch Number 4400 -> Loss: 0.051 Accuracy: 0.053
 - Batch Number 4500 -> Loss: 0.050 Accuracy: 0.053
 - Batch Number 4600 -> Loss: 0.049 Accuracy: 0.053
 - Batch Number 4700 -> Loss: 0.048 Accuracy: 0.054
 - Batch Number 4800 -> Loss: 0.047 Accuracy: 0.053
 - Batch Number 4900 -> Loss: 0.046 Accuracy: 0.055
 - Batch Number 5000 -> Loss: 0.045 Accuracy: 0.055
 - Batch Number 5100 -> Loss: 0.045 Accuracy: 0.056
 - Batch Number 5200 -> Loss: 0.044 Accuracy: 0.057
 - Batch Number 5300 -> Loss: 0.042 Accuracy: 0.058
 - Batch Number 5400 -> Loss: 0.042 Accuracy: 0.059
 - Batch Number 5500 -> Loss: 0.041 Accuracy: 0.061
 - Batch Number 5600 -> Loss: 0.040 Accuracy: 0.062
 - Batch Number 5700 -> Loss: 0.040 Accuracy: 0.063
 - Batch Number 5800 -> Loss: 0.039 Accuracy: 0.064
 - Batch Number 5900 -> Loss: 0.038 Accuracy: 0.065
 - Batch Number 6000 -> Loss: 0.038 Accuracy: 0.066
 - Batch Number 6100 -> Loss: 0.037 Accuracy: 0.066
 - Batch Number 6200 -> Loss: 0.036 Accuracy: 0.066
 - Batch Number 6300 -> Loss: 0.036 Accuracy: 0.067
 - Batch Number 6400 -> Loss: 0.036 Accuracy: 0.067
 - Batch Number 6500 -> Loss: 0.035 Accuracy: 0.068
 - Batch Number 6600 -> Loss: 0.034 Accuracy: 0.069
 - Batch Number 6700 -> Loss: 0.034 Accuracy: 0.070
 - Batch Number 6800 -> Loss: 0.033 Accuracy: 0.070
 - Batch Number 6900 -> Loss: 0.033 Accuracy: 0.071
 - Batch Number 7000 -> Loss: 0.032 Accuracy: 0.072
 - Batch Number 7100 -> Loss: 0.032 Accuracy: 0.073
 - Batch Number 7200 -> Loss: 0.031 Accuracy: 0.073
 - Batch Number 7300 -> Loss: 0.031 Accuracy: 0.074
 - Batch Number 7400 -> Loss: 0.030 Accuracy: 0.075
 - Batch Number 7500 -> Loss: 0.030 Accuracy: 0.075
 - Batch Number 7600 -> Loss: 0.030 Accuracy: 0.076
 - Batch Number 7700 -> Loss: 0.029 Accuracy: 0.077
 - Batch Number 7800 -> Loss: 0.029 Accuracy: 0.078
 - Batch Number 7900 -> Loss: 0.028 Accuracy: 0.079
 - Batch Number 8000 -> Loss: 0.028 Accuracy: 0.080
 - Batch Number 8100 -> Loss: 0.028 Accuracy: 0.080
 - Batch Number 8200 -> Loss: 0.027 Accuracy: 0.081
 - Batch Number 8300 -> Loss: 0.027 Accuracy: 0.082
 - Batch Number 8400 -> Loss: 0.027 Accuracy: 0.083
 - Batch Number 8500 -> Loss: 0.027 Accuracy: 0.083
 - Batch Number 8600 -> Loss: 0.026 Accuracy: 0.084
 - Batch Number 8700 -> Loss: 0.026 Accuracy: 0.084
 - Batch Number 8800 -> Loss: 0.026 Accuracy: 0.085
 - Batch Number 8900 -> Loss: 0.025 Accuracy: 0.086
 - Batch Number 9000 -> Loss: 0.025 Accuracy: 0.086
 - Batch Number 9100 -> Loss: 0.025 Accuracy: 0.087
 - Batch Number 9200 -> Loss: 0.024 Accuracy: 0.087
 - Batch Number 9300 -> Loss: 0.024 Accuracy: 0.087
 - Batch Number 9400 -> Loss: 0.024 Accuracy: 0.087
 - Batch Number 9500 -> Loss: 0.024 Accuracy: 0.088
 - Batch Number 9600 -> Loss: 0.023 Accuracy: 0.088
 - Batch Number 9700 -> Loss: 0.023 Accuracy: 0.088
 - Batch Number 9800 -> Loss: 0.023 Accuracy: 0.089
 - Batch Number 9900 -> Loss: 0.023 Accuracy: 0.090
 - Batch Number 10000 -> Loss: 0.023 Accuracy: 0.090
 - Batch Number 10100 -> Loss: 0.022 Accuracy: 0.091
 - Batch Number 10200 -> Loss: 0.022 Accuracy: 0.091
 - Batch Number 10300 -> Loss: 0.022 Accuracy: 0.091
 - Batch Number 10400 -> Loss: 0.022 Accuracy: 0.092
 - Batch Number 10500 -> Loss: 0.021 Accuracy: 0.092
 - Batch Number 10600 -> Loss: 0.021 Accuracy: 0.092
 - Batch Number 10700 -> Loss: 0.021 Accuracy: 0.093
 - Batch Number 10800 -> Loss: 0.021 Accuracy: 0.093
 - Batch Number 10900 -> Loss: 0.021 Accuracy: 0.093
 - Batch Number 11000 -> Loss: 0.020 Accuracy: 0.093
/home-net/omartinez/TFG/attention_zoo/attentions/skyformer/skyformer.py:178: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.random_sign = torch.tensor(self.random_sign, device=Q.device)
/home-net/omartinez/TFG/attention_zoo/attentions/skyformer/skyformer.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  random_sign = torch.tensor(random_sign, device=X1.device)
Error executing job with overrides: ['model=model_v2', 'training.EPOCHS=4', 'training.GPU=2']
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: \ 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: | 0.006 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: / 0.006 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: - 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:      train_batches/batch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  train_batches/train_acc ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██████
wandb: train_batches/train_loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      train_batches/batch 11000
wandb:  train_batches/train_acc 0.09291
wandb: train_batches/train_loss 0.02048
wandb: 
wandb: 🚀 View run holographic-droid-49 at: https://wandb.ai/tfg_oriol/action_classification/runs/31k69y5g
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230504_110419-31k69y5g/logs
Traceback (most recent call last):
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1133, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/usr/lib/python3.8/queue.py", line 179, in get
    self.not_empty.wait(remaining)
  File "/usr/lib/python3.8/threading.py", line 306, in wait
    gotit = waiter.acquire(True, timeout)
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1925894) is killed by signal: Killed. 

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "main.py", line 147, in <module>
    run_experiment()
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "main.py", line 125, in run_experiment
    trained_model = train_model(model, dataloaders, criterion, optimizer, DEVICE, num_epochs, print_batch)
  File "/home-net/omartinez/TFG/train.py", line 36, in train_model
    for i, (inputs, labels) in enumerate(dataloader):
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 634, in __next__
    data = self._next_data()
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1329, in _next_data
    idx, data = self._get_data()
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1285, in _get_data
    success, data = self._try_get_data()
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1146, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 1925894) exited unexpectedly

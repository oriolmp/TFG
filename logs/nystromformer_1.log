Training starting...
Datetime: 2023-05-04 11:04:53.072424
wandb: Currently logged in as: oriolmartinez (tfg_oriol). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /home-net/omartinez/TFG/wandb/run-20230504_110457-taad9h6j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run old-council-50
wandb: â­ï¸ View project at https://wandb.ai/tfg_oriol/action_classification
wandb: ğŸš€ View run at https://wandb.ai/tfg_oriol/action_classification/runs/taad9h6j
NAME: action_classification
dataset:
  NAME: epic_kitchens
  FRAME_SIZE: 112
  NUM_FRAMES: 100
  IN_CHANNELS: 3
model:
  ATTENTION: nystromformer
  eps: 1.0e-08
  num_landmarks: 64
  pinv_iterations: 64
  NUM_CLASSES: 96
  PATCH_SIZE: 16
  DEPTH: 2
  HEADS: 4
training:
  EPOCHS: 4
  SEED: 0
  BATCH_SIZE: 2
  DATA_THREADS: 5
  PRINT_BATCH: 100
  LEARNING_RATE: 1.0e-06
  PRETRAINED_STATE_PATH: None
  GPU: 3
inference:
  WEIGHTS_PATH: /home-net/omartinez/TFG/weights/
  MODEL: vanilla_attention_1

Loading the data...
Loading the training dataset
Loading the validation dataset
Epoch 0/3
----------
 - Batch Number 100 -> Loss: nan Accuracy: 0.165
 - Batch Number 200 -> Loss: nan Accuracy: 0.158
 - Batch Number 300 -> Loss: nan Accuracy: 0.182
 - Batch Number 400 -> Loss: nan Accuracy: 0.195
 - Batch Number 500 -> Loss: nan Accuracy: 0.209
 - Batch Number 600 -> Loss: nan Accuracy: 0.213
 - Batch Number 700 -> Loss: nan Accuracy: 0.206
 - Batch Number 800 -> Loss: nan Accuracy: 0.201
 - Batch Number 900 -> Loss: nan Accuracy: 0.203
 - Batch Number 1000 -> Loss: nan Accuracy: 0.205
 - Batch Number 1100 -> Loss: nan Accuracy: 0.203
 - Batch Number 1200 -> Loss: nan Accuracy: 0.208
 - Batch Number 1300 -> Loss: nan Accuracy: 0.206
 - Batch Number 1400 -> Loss: nan Accuracy: 0.206
 - Batch Number 1500 -> Loss: nan Accuracy: 0.208
 - Batch Number 1600 -> Loss: nan Accuracy: 0.210
 - Batch Number 1700 -> Loss: nan Accuracy: 0.212
 - Batch Number 1800 -> Loss: nan Accuracy: 0.211
 - Batch Number 1900 -> Loss: nan Accuracy: 0.211
 - Batch Number 2000 -> Loss: nan Accuracy: 0.214
 - Batch Number 2100 -> Loss: nan Accuracy: 0.212
 - Batch Number 2200 -> Loss: nan Accuracy: 0.214
 - Batch Number 2300 -> Loss: nan Accuracy: 0.216
 - Batch Number 2400 -> Loss: nan Accuracy: 0.216
 - Batch Number 2500 -> Loss: nan Accuracy: 0.217
 - Batch Number 2600 -> Loss: nan Accuracy: 0.217
 - Batch Number 2700 -> Loss: nan Accuracy: 0.216
 - Batch Number 2800 -> Loss: nan Accuracy: 0.216
 - Batch Number 2900 -> Loss: nan Accuracy: 0.217
 - Batch Number 3000 -> Loss: nan Accuracy: 0.217
 - Batch Number 3100 -> Loss: nan Accuracy: 0.218
 - Batch Number 3200 -> Loss: nan Accuracy: 0.217
 - Batch Number 3300 -> Loss: nan Accuracy: 0.216
 - Batch Number 3400 -> Loss: nan Accuracy: 0.217
 - Batch Number 3500 -> Loss: nan Accuracy: 0.217
 - Batch Number 3600 -> Loss: nan Accuracy: 0.218
 - Batch Number 3700 -> Loss: nan Accuracy: 0.217
 - Batch Number 3800 -> Loss: nan Accuracy: 0.217
 - Batch Number 3900 -> Loss: nan Accuracy: 0.217
 - Batch Number 4000 -> Loss: nan Accuracy: 0.217
 - Batch Number 4100 -> Loss: nan Accuracy: 0.217
 - Batch Number 4200 -> Loss: nan Accuracy: 0.217
 - Batch Number 4300 -> Loss: nan Accuracy: 0.218
 - Batch Number 4400 -> Loss: nan Accuracy: 0.218
 - Batch Number 4500 -> Loss: nan Accuracy: 0.218
 - Batch Number 4600 -> Loss: nan Accuracy: 0.219
 - Batch Number 4700 -> Loss: nan Accuracy: 0.220
 - Batch Number 4800 -> Loss: nan Accuracy: 0.220
 - Batch Number 4900 -> Loss: nan Accuracy: 0.220
 - Batch Number 5000 -> Loss: nan Accuracy: 0.220
 - Batch Number 5100 -> Loss: nan Accuracy: 0.219
 - Batch Number 5200 -> Loss: nan Accuracy: 0.219
 - Batch Number 5300 -> Loss: nan Accuracy: 0.219
 - Batch Number 5400 -> Loss: nan Accuracy: 0.219
 - Batch Number 5500 -> Loss: nan Accuracy: 0.219
 - Batch Number 5600 -> Loss: nan Accuracy: 0.219
 - Batch Number 5700 -> Loss: nan Accuracy: 0.220
 - Batch Number 5800 -> Loss: nan Accuracy: 0.221
 - Batch Number 5900 -> Loss: nan Accuracy: 0.220
 - Batch Number 6000 -> Loss: nan Accuracy: 0.221
 - Batch Number 6100 -> Loss: nan Accuracy: 0.220
 - Batch Number 6200 -> Loss: nan Accuracy: 0.221
 - Batch Number 6300 -> Loss: nan Accuracy: 0.221
 - Batch Number 6400 -> Loss: nan Accuracy: 0.221
 - Batch Number 6500 -> Loss: nan Accuracy: 0.220
 - Batch Number 6600 -> Loss: nan Accuracy: 0.220
 - Batch Number 6700 -> Loss: nan Accuracy: 0.219
 - Batch Number 6800 -> Loss: nan Accuracy: 0.218
 - Batch Number 6900 -> Loss: nan Accuracy: 0.219
 - Batch Number 7000 -> Loss: nan Accuracy: 0.219
 - Batch Number 7100 -> Loss: nan Accuracy: 0.219
 - Batch Number 7200 -> Loss: nan Accuracy: 0.219
 - Batch Number 7300 -> Loss: nan Accuracy: 0.219
 - Batch Number 7400 -> Loss: nan Accuracy: 0.219
 - Batch Number 7500 -> Loss: nan Accuracy: 0.218
 - Batch Number 7600 -> Loss: nan Accuracy: 0.218
 - Batch Number 7700 -> Loss: nan Accuracy: 0.217
 - Batch Number 7800 -> Loss: nan Accuracy: 0.217
 - Batch Number 7900 -> Loss: nan Accuracy: 0.218
Error executing job with overrides: ['model=model_v3', 'training.EPOCHS=4', 'training.GPU=3']
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: \ 0.006 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: | 0.006 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: / 0.015 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:     train_batches/batch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: train_batches/train_acc â–â–ƒâ–‡â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:      train_batches/batch 7900
wandb:  train_batches/train_acc 0.21804
wandb: train_batches/train_loss nan
wandb: 
wandb: ğŸš€ View run old-council-50 at: https://wandb.ai/tfg_oriol/action_classification/runs/taad9h6j
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230504_110457-taad9h6j/logs
Traceback (most recent call last):
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1133, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/usr/lib/python3.8/queue.py", line 179, in get
    self.not_empty.wait(remaining)
  File "/usr/lib/python3.8/threading.py", line 306, in wait
    gotit = waiter.acquire(True, timeout)
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1926814) is killed by signal: Killed. 

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "main.py", line 147, in <module>
    run_experiment()
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "main.py", line 125, in run_experiment
    trained_model = train_model(model, dataloaders, criterion, optimizer, DEVICE, num_epochs, print_batch)
  File "/home-net/omartinez/TFG/train.py", line 36, in train_model
    for i, (inputs, labels) in enumerate(dataloader):
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 634, in __next__
    data = self._next_data()
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1329, in _next_data
    idx, data = self._get_data()
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1285, in _get_data
    success, data = self._try_get_data()
  File "/home-net/omartinez/TFG/TFG_venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1146, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 1926814) exited unexpectedly

Training starting...
Datetime: 2023-05-09 11:02:16.934905
wandb: Currently logged in as: oriolmartinez (tfg_oriol). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.15.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.0
wandb: Run data is saved locally in /home-net/omartinez/TFG/wandb/run-20230509_110307-0k0t20or
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sponge-64
wandb: ⭐️ View project at https://wandb.ai/tfg_oriol/action_classification
wandb: 🚀 View run at https://wandb.ai/tfg_oriol/action_classification/runs/0k0t20or
NAME: action_classification
dataset:
  NAME: epic_kitchens
  FRAME_SIZE: 112
  NUM_FRAMES: 100
  IN_CHANNELS: 3
model:
  ATTENTION: vanilla_attention
  NUM_CLASSES: 96
  PATCH_SIZE: 16
  DEPTH: 2
  HEADS: 4
training:
  EPOCHS: 1
  SEED: 0
  BATCH_SIZE: 4
  DATA_THREADS: 5
  PRINT_BATCH: 50
  LEARNING_RATE: 1.0e-05
  PRETRAINED_STATE_PATH: None
  GPU: 1
inference:
  WEIGHTS_PATH: /home-net/omartinez/TFG/weights/
  MODEL: vanilla_attention_1

Loading the data...
Loading the training dataset
Loading the validation dataset
Epoch 0/0
----------
 - Batch Number 50 -> Loss: 4.541 Accuracy: 0.016
 - Batch Number 100 -> Loss: 4.492 Accuracy: 0.023
 - Batch Number 150 -> Loss: 4.619 Accuracy: 0.030
 - Batch Number 200 -> Loss: 4.676 Accuracy: 0.023
 - Batch Number 250 -> Loss: 4.604 Accuracy: 0.033
 - Batch Number 300 -> Loss: 4.172 Accuracy: 0.052
 - Batch Number 350 -> Loss: 4.400 Accuracy: 0.046
 - Batch Number 400 -> Loss: 4.572 Accuracy: 0.013
 - Batch Number 450 -> Loss: 4.237 Accuracy: 0.066
 - Batch Number 500 -> Loss: 4.628 Accuracy: 0.034
 - Batch Number 550 -> Loss: 4.483 Accuracy: 0.021
 - Batch Number 600 -> Loss: 4.421 Accuracy: 0.030
 - Batch Number 650 -> Loss: 4.578 Accuracy: 0.046
 - Batch Number 700 -> Loss: 4.208 Accuracy: 0.038
 - Batch Number 750 -> Loss: 4.310 Accuracy: 0.042
 - Batch Number 800 -> Loss: 4.279 Accuracy: 0.030
 - Batch Number 850 -> Loss: 4.562 Accuracy: 0.040
 - Batch Number 900 -> Loss: 4.166 Accuracy: 0.045
 - Batch Number 950 -> Loss: 4.784 Accuracy: 0.108
 - Batch Number 1000 -> Loss: 4.408 Accuracy: 0.050
 - Batch Number 1050 -> Loss: 4.457 Accuracy: 0.041
 - Batch Number 1100 -> Loss: 4.543 Accuracy: 0.021
 - Batch Number 1150 -> Loss: 4.420 Accuracy: 0.019
 - Batch Number 1200 -> Loss: 4.640 Accuracy: 0.031
 - Batch Number 1250 -> Loss: 4.768 Accuracy: 0.065
 - Batch Number 1300 -> Loss: 4.533 Accuracy: 0.030
 - Batch Number 1350 -> Loss: 4.522 Accuracy: 0.058
 - Batch Number 1400 -> Loss: 4.638 Accuracy: 0.056
 - Batch Number 1450 -> Loss: 4.570 Accuracy: 0.040
 - Batch Number 1500 -> Loss: 4.384 Accuracy: 0.035
 - Batch Number 1550 -> Loss: 4.484 Accuracy: 0.047
 - Batch Number 1600 -> Loss: 4.121 Accuracy: 0.023
 - Batch Number 1650 -> Loss: 4.588 Accuracy: 0.043
 - Batch Number 1700 -> Loss: 4.414 Accuracy: 0.037
 - Batch Number 1750 -> Loss: 4.680 Accuracy: 0.027
 - Batch Number 1800 -> Loss: 4.678 Accuracy: 0.049
 - Batch Number 1850 -> Loss: 4.271 Accuracy: 0.034
 - Batch Number 1900 -> Loss: 5.072 Accuracy: 0.063
 - Batch Number 1950 -> Loss: 3.994 Accuracy: 0.023
 - Batch Number 2000 -> Loss: 4.593 Accuracy: 0.019
 - Batch Number 2050 -> Loss: 4.556 Accuracy: 0.023
 - Batch Number 2100 -> Loss: 4.497 Accuracy: 0.045
 - Batch Number 2150 -> Loss: 4.723 Accuracy: 0.035
 - Batch Number 2200 -> Loss: 4.726 Accuracy: 0.015
 - Batch Number 2250 -> Loss: 4.601 Accuracy: 0.059
 - Batch Number 2300 -> Loss: 4.559 Accuracy: 0.034
 - Batch Number 2350 -> Loss: 3.952 Accuracy: 0.028
 - Batch Number 2400 -> Loss: 4.439 Accuracy: 0.061
 - Batch Number 2450 -> Loss: 4.547 Accuracy: 0.021
 - Batch Number 2500 -> Loss: 4.611 Accuracy: 0.042
 - Batch Number 2550 -> Loss: 4.417 Accuracy: 0.047
 - Batch Number 2600 -> Loss: 4.581 Accuracy: 0.028
 - Batch Number 2650 -> Loss: 4.293 Accuracy: 0.030
 - Batch Number 2700 -> Loss: 4.278 Accuracy: 0.057
 - Batch Number 2750 -> Loss: 4.434 Accuracy: 0.036
 - Batch Number 2800 -> Loss: 4.294 Accuracy: 0.025
 - Batch Number 2850 -> Loss: 4.832 Accuracy: 0.073
 - Batch Number 2900 -> Loss: 4.592 Accuracy: 0.029
 - Batch Number 2950 -> Loss: 4.902 Accuracy: 0.131
 - Batch Number 3000 -> Loss: 4.601 Accuracy: 0.034
 - Batch Number 3050 -> Loss: 4.498 Accuracy: 0.041
 - Batch Number 3100 -> Loss: 4.525 Accuracy: 0.039
 - Batch Number 3150 -> Loss: 4.588 Accuracy: 0.049
 - Batch Number 3200 -> Loss: 4.713 Accuracy: 0.027
 - Batch Number 3250 -> Loss: 4.601 Accuracy: 0.042
 - Batch Number 3300 -> Loss: 4.224 Accuracy: 0.027
 - Batch Number 3350 -> Loss: 4.747 Accuracy: 0.028
 - Batch Number 3400 -> Loss: 4.483 Accuracy: 0.032
 - Batch Number 3450 -> Loss: 4.507 Accuracy: 0.058
 - Batch Number 3500 -> Loss: 4.119 Accuracy: 0.027
 - Batch Number 3550 -> Loss: 4.595 Accuracy: 0.043
 - Batch Number 3600 -> Loss: 4.524 Accuracy: 0.033
 - Batch Number 3650 -> Loss: 4.397 Accuracy: 0.057
 - Batch Number 3700 -> Loss: 4.311 Accuracy: 0.052
 - Batch Number 3750 -> Loss: 4.603 Accuracy: 0.041
 - Batch Number 3800 -> Loss: 4.495 Accuracy: 0.051
 - Batch Number 3850 -> Loss: 3.996 Accuracy: 0.031
 - Batch Number 3900 -> Loss: 4.517 Accuracy: 0.035
 - Batch Number 3950 -> Loss: 4.171 Accuracy: 0.041
 - Batch Number 4000 -> Loss: 4.587 Accuracy: 0.019
 - Batch Number 4050 -> Loss: 4.382 Accuracy: 0.059
 - Batch Number 4100 -> Loss: 4.582 Accuracy: 0.050
 - Batch Number 4150 -> Loss: 4.369 Accuracy: 0.037
 - Batch Number 4200 -> Loss: 4.652 Accuracy: 0.044
 - Batch Number 4250 -> Loss: 5.177 Accuracy: 0.017
 - Batch Number 4300 -> Loss: 4.787 Accuracy: 0.060
 - Batch Number 4350 -> Loss: 4.262 Accuracy: 0.047
 - Batch Number 4400 -> Loss: 4.524 Accuracy: 0.037
 - Batch Number 4450 -> Loss: 4.560 Accuracy: 0.058
 - Batch Number 4500 -> Loss: 4.251 Accuracy: 0.028
 - Batch Number 4550 -> Loss: 4.687 Accuracy: 0.042
 - Batch Number 4600 -> Loss: 4.655 Accuracy: 0.076
 - Batch Number 4650 -> Loss: 4.577 Accuracy: 0.028
 - Batch Number 4700 -> Loss: 4.510 Accuracy: 0.036
 - Batch Number 4750 -> Loss: 4.605 Accuracy: 0.030
 - Batch Number 4800 -> Loss: 4.292 Accuracy: 0.034
 - Batch Number 4850 -> Loss: 4.462 Accuracy: 0.017
 - Batch Number 4900 -> Loss: 4.362 Accuracy: 0.020
 - Batch Number 4950 -> Loss: 4.380 Accuracy: 0.056
 - Batch Number 5000 -> Loss: 4.373 Accuracy: 0.025
 - Batch Number 5050 -> Loss: 3.956 Accuracy: 0.071
 - Batch Number 5100 -> Loss: 4.377 Accuracy: 0.067
 - Batch Number 5150 -> Loss: 4.441 Accuracy: 0.048
 - Batch Number 5200 -> Loss: 4.692 Accuracy: 0.045
 - Batch Number 5250 -> Loss: 4.360 Accuracy: 0.065
 - Batch Number 5300 -> Loss: 4.364 Accuracy: 0.092
 - Batch Number 5350 -> Loss: 4.582 Accuracy: 0.026
 - Batch Number 5400 -> Loss: 4.514 Accuracy: 0.031
 - Batch Number 5450 -> Loss: 4.477 Accuracy: 0.021
 - Batch Number 5500 -> Loss: 4.523 Accuracy: 0.024
 - Batch Number 5550 -> Loss: 4.066 Accuracy: 0.035
 - Batch Number 5600 -> Loss: 4.692 Accuracy: 0.039
 - Batch Number 5650 -> Loss: 4.445 Accuracy: 0.028
 - Batch Number 5700 -> Loss: 4.873 Accuracy: 0.028
 - Batch Number 5750 -> Loss: 4.729 Accuracy: 0.091
 - Batch Number 5800 -> Loss: 4.562 Accuracy: 0.047
 - Batch Number 5850 -> Loss: 4.240 Accuracy: 0.024
 - Batch Number 5900 -> Loss: 4.642 Accuracy: 0.031
 - Batch Number 5950 -> Loss: 4.716 Accuracy: 0.021
 - Batch Number 6000 -> Loss: 4.486 Accuracy: 0.022
 - Batch Number 6050 -> Loss: 4.683 Accuracy: 0.033
 - Batch Number 6100 -> Loss: 4.748 Accuracy: 0.039
 - Batch Number 6150 -> Loss: 4.281 Accuracy: 0.046
 - Batch Number 6200 -> Loss: 4.064 Accuracy: 0.032
 - Batch Number 6250 -> Loss: 4.647 Accuracy: 0.063
 - Batch Number 6300 -> Loss: 4.594 Accuracy: 0.021
 - Batch Number 6350 -> Loss: 4.667 Accuracy: 0.037
 - Batch Number 6400 -> Loss: 4.592 Accuracy: 0.033
 - Batch Number 6450 -> Loss: 4.684 Accuracy: 0.021
 - Batch Number 6500 -> Loss: 4.631 Accuracy: 0.043
 - Batch Number 6550 -> Loss: 4.709 Accuracy: 0.092
 - Batch Number 6600 -> Loss: 4.648 Accuracy: 0.049
 - Batch Number 6650 -> Loss: 4.734 Accuracy: 0.037
 - Batch Number 6700 -> Loss: 4.794 Accuracy: 0.061
 - Batch Number 6750 -> Loss: 4.733 Accuracy: 0.030
 - Batch Number 6800 -> Loss: 4.354 Accuracy: 0.030
 - Batch Number 6850 -> Loss: 4.336 Accuracy: 0.056
 - Batch Number 6900 -> Loss: 4.507 Accuracy: 0.104
 - Batch Number 6950 -> Loss: 4.670 Accuracy: 0.053
 - Batch Number 7000 -> Loss: 4.649 Accuracy: 0.045
 - Batch Number 7050 -> Loss: 4.642 Accuracy: 0.038
 - Batch Number 7100 -> Loss: 4.096 Accuracy: 0.069
 - Batch Number 7150 -> Loss: 4.576 Accuracy: 0.069
 - Batch Number 7200 -> Loss: 4.635 Accuracy: 0.062
 - Batch Number 7250 -> Loss: 4.425 Accuracy: 0.022
 - Batch Number 7300 -> Loss: 4.238 Accuracy: 0.055
 - Batch Number 7350 -> Loss: 4.390 Accuracy: 0.031
 - Batch Number 7400 -> Loss: 3.948 Accuracy: 0.042
 - Batch Number 7450 -> Loss: 4.361 Accuracy: 0.027
 - Batch Number 7500 -> Loss: 4.710 Accuracy: 0.029
 - Batch Number 7550 -> Loss: 4.621 Accuracy: 0.047
 - Batch Number 7600 -> Loss: 4.043 Accuracy: 0.039
 - Batch Number 7650 -> Loss: 4.718 Accuracy: 0.069
 - Batch Number 7700 -> Loss: 4.558 Accuracy: 0.044
 - Batch Number 7750 -> Loss: 4.419 Accuracy: 0.032
 - Batch Number 7800 -> Loss: 4.908 Accuracy: 0.045
 - Batch Number 7850 -> Loss: 4.445 Accuracy: 0.040
 - Batch Number 7900 -> Loss: 4.704 Accuracy: 0.080
 - Batch Number 7950 -> Loss: 4.659 Accuracy: 0.038
 - Batch Number 8000 -> Loss: 4.570 Accuracy: 0.043
 - Batch Number 8050 -> Loss: 4.834 Accuracy: 0.040
 - Batch Number 8100 -> Loss: 4.244 Accuracy: 0.047
 - Batch Number 8150 -> Loss: 4.638 Accuracy: 0.028
 - Batch Number 8200 -> Loss: 4.565 Accuracy: 0.080
 - Batch Number 8250 -> Loss: 4.444 Accuracy: 0.019
 - Batch Number 8300 -> Loss: 4.645 Accuracy: 0.025
 - Batch Number 8350 -> Loss: 4.704 Accuracy: 0.058
 - Batch Number 8400 -> Loss: 4.517 Accuracy: 0.032
 - Batch Number 8450 -> Loss: 4.475 Accuracy: 0.037
 - Batch Number 8500 -> Loss: 4.421 Accuracy: 0.048
 - Batch Number 8550 -> Loss: 4.679 Accuracy: 0.089
 - Batch Number 8600 -> Loss: 4.606 Accuracy: 0.063
 - Batch Number 8650 -> Loss: 4.552 Accuracy: 0.080
 - Batch Number 8700 -> Loss: 4.559 Accuracy: 0.056
 - Batch Number 8750 -> Loss: 4.699 Accuracy: 0.040
 - Batch Number 8800 -> Loss: 4.912 Accuracy: 0.042
 - Batch Number 8850 -> Loss: 4.576 Accuracy: 0.053
 - Batch Number 8900 -> Loss: 4.382 Accuracy: 0.055
 - Batch Number 8950 -> Loss: 4.584 Accuracy: 0.068
 - Batch Number 9000 -> Loss: 4.320 Accuracy: 0.066
 - Batch Number 9050 -> Loss: 4.949 Accuracy: 0.024
 - Batch Number 9100 -> Loss: 4.427 Accuracy: 0.033
 - Batch Number 9150 -> Loss: 4.025 Accuracy: 0.050
 - Batch Number 9200 -> Loss: 4.149 Accuracy: 0.066
 - Batch Number 9250 -> Loss: 4.594 Accuracy: 0.065
 - Batch Number 9300 -> Loss: 4.763 Accuracy: 0.089
 - Batch Number 9350 -> Loss: 4.578 Accuracy: 0.035
 - Batch Number 9400 -> Loss: 4.632 Accuracy: 0.055
 - Batch Number 9450 -> Loss: 4.508 Accuracy: 0.036
 - Batch Number 9500 -> Loss: 4.308 Accuracy: 0.047
 - Batch Number 9550 -> Loss: 4.504 Accuracy: 0.044
 - Batch Number 9600 -> Loss: 4.725 Accuracy: 0.050
 - Batch Number 9650 -> Loss: 4.656 Accuracy: 0.061
 - Batch Number 9700 -> Loss: 4.585 Accuracy: 0.035
 - Batch Number 9750 -> Loss: 4.351 Accuracy: 0.029
 - Batch Number 9800 -> Loss: 4.817 Accuracy: 0.052
 - Batch Number 9850 -> Loss: 4.234 Accuracy: 0.066
 - Batch Number 9900 -> Loss: 4.275 Accuracy: 0.037
 - Batch Number 9950 -> Loss: 4.450 Accuracy: 0.033
 - Batch Number 10000 -> Loss: 4.649 Accuracy: 0.074
 - Batch Number 10050 -> Loss: 4.658 Accuracy: 0.052
 - Batch Number 10100 -> Loss: 4.297 Accuracy: 0.056
 - Batch Number 10150 -> Loss: 4.567 Accuracy: 0.057
 - Batch Number 10200 -> Loss: 4.511 Accuracy: 0.055
 - Batch Number 10250 -> Loss: 4.538 Accuracy: 0.060
 - Batch Number 10300 -> Loss: 4.648 Accuracy: 0.059
 - Batch Number 10350 -> Loss: 4.720 Accuracy: 0.028
 - Batch Number 10400 -> Loss: 4.621 Accuracy: 0.027
 - Batch Number 10450 -> Loss: 4.690 Accuracy: 0.047
 - Batch Number 10500 -> Loss: 4.271 Accuracy: 0.046
 - Batch Number 10550 -> Loss: 4.455 Accuracy: 0.038
 - Batch Number 10600 -> Loss: 4.684 Accuracy: 0.030
 - Batch Number 10650 -> Loss: 4.615 Accuracy: 0.013
 - Batch Number 10700 -> Loss: 4.569 Accuracy: 0.027
 - Batch Number 10750 -> Loss: 4.485 Accuracy: 0.032
 - Batch Number 10800 -> Loss: 4.250 Accuracy: 0.068
 - Batch Number 10850 -> Loss: 4.383 Accuracy: 0.068
 - Batch Number 10900 -> Loss: 4.541 Accuracy: 0.035
 - Batch Number 10950 -> Loss: 4.661 Accuracy: 0.028
 - Batch Number 11000 -> Loss: 4.503 Accuracy: 0.063
 - Batch Number 11050 -> Loss: 4.505 Accuracy: 0.043
 - Batch Number 11100 -> Loss: 4.587 Accuracy: 0.037
 - Batch Number 11150 -> Loss: 4.724 Accuracy: 0.018
 - Batch Number 11200 -> Loss: 4.652 Accuracy: 0.021
 - Batch Number 11250 -> Loss: 4.277 Accuracy: 0.034
 - Batch Number 11300 -> Loss: 4.551 Accuracy: 0.042
 - Batch Number 11350 -> Loss: 4.771 Accuracy: 0.042
 - Batch Number 11400 -> Loss: 3.942 Accuracy: 0.050
 - Batch Number 11450 -> Loss: 4.803 Accuracy: 0.037
 - Batch Number 11500 -> Loss: 4.530 Accuracy: 0.034
 - Batch Number 11550 -> Loss: 4.584 Accuracy: 0.027
 - Batch Number 11600 -> Loss: 4.558 Accuracy: 0.035
 - Batch Number 11650 -> Loss: 4.649 Accuracy: 0.044
 - Batch Number 11700 -> Loss: 4.021 Accuracy: 0.062
 - Batch Number 11750 -> Loss: 4.620 Accuracy: 0.027
 - Batch Number 11800 -> Loss: 4.075 Accuracy: 0.062
 - Batch Number 11850 -> Loss: 4.824 Accuracy: 0.020
 - Batch Number 11900 -> Loss: 4.226 Accuracy: 0.041
 - Batch Number 11950 -> Loss: 4.583 Accuracy: 0.064
 - Batch Number 12000 -> Loss: 4.288 Accuracy: 0.016
 - Batch Number 12050 -> Loss: 4.743 Accuracy: 0.022
 - Batch Number 12100 -> Loss: 4.310 Accuracy: 0.058
 - Batch Number 12150 -> Loss: 4.239 Accuracy: 0.051
 - Batch Number 12200 -> Loss: 4.274 Accuracy: 0.025
 - Batch Number 12250 -> Loss: 3.985 Accuracy: 0.041
 - Batch Number 12300 -> Loss: 4.384 Accuracy: 0.041
 - Batch Number 12350 -> Loss: 4.806 Accuracy: 0.054
 - Batch Number 12400 -> Loss: 4.523 Accuracy: 0.061
 - Batch Number 12450 -> Loss: 4.858 Accuracy: 0.028
 - Batch Number 12500 -> Loss: 4.857 Accuracy: 0.030
 - Batch Number 12550 -> Loss: 4.666 Accuracy: 0.042
 - Batch Number 12600 -> Loss: 4.488 Accuracy: 0.043
 - Batch Number 12650 -> Loss: 4.281 Accuracy: 0.034
 - Batch Number 12700 -> Loss: 4.478 Accuracy: 0.041
 - Batch Number 12750 -> Loss: 4.121 Accuracy: 0.035
 - Batch Number 12800 -> Loss: 4.183 Accuracy: 0.013
 - Batch Number 12850 -> Loss: 4.714 Accuracy: 0.046
 - Batch Number 12900 -> Loss: 3.961 Accuracy: 0.055
 - Batch Number 12950 -> Loss: 4.668 Accuracy: 0.077
 - Batch Number 13000 -> Loss: 4.609 Accuracy: 0.037
 - Batch Number 13050 -> Loss: 4.429 Accuracy: 0.042
 - Batch Number 13100 -> Loss: 4.638 Accuracy: 0.062
 - Batch Number 13150 -> Loss: 4.214 Accuracy: 0.068
 - Batch Number 13200 -> Loss: 4.386 Accuracy: 0.078
 - Batch Number 13250 -> Loss: 4.790 Accuracy: 0.045
 - Batch Number 13300 -> Loss: 4.104 Accuracy: 0.059
 - Batch Number 13350 -> Loss: 4.388 Accuracy: 0.033
 - Batch Number 13400 -> Loss: 4.627 Accuracy: 0.041
 - Batch Number 13450 -> Loss: 4.623 Accuracy: 0.059
 - Batch Number 13500 -> Loss: 4.250 Accuracy: 0.031
 - Batch Number 13550 -> Loss: 4.502 Accuracy: 0.022
 - Batch Number 13600 -> Loss: 4.648 Accuracy: 0.065
 - Batch Number 13650 -> Loss: 4.709 Accuracy: 0.057
 - Batch Number 13700 -> Loss: 4.274 Accuracy: 0.061
 - Batch Number 13750 -> Loss: 4.155 Accuracy: 0.033
 - Batch Number 13800 -> Loss: 4.319 Accuracy: 0.027
 - Batch Number 13850 -> Loss: 4.654 Accuracy: 0.041
 - Batch Number 13900 -> Loss: 4.385 Accuracy: 0.049
 - Batch Number 13950 -> Loss: 4.263 Accuracy: 0.049
 - Batch Number 14000 -> Loss: 4.326 Accuracy: 0.091
 - Batch Number 14050 -> Loss: 4.372 Accuracy: 0.022
 - Batch Number 14100 -> Loss: 4.344 Accuracy: 0.043
 - Batch Number 14150 -> Loss: 4.561 Accuracy: 0.051
 - Batch Number 14200 -> Loss: 4.730 Accuracy: 0.048
train Loss: 4.4771 Acc: 0.0176
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:      train_batches/batch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:  train_batches/train_acc ▂▁▃▃▂▆▁▃▄▃▃▂▄▄▅▃▅▃▆▅▇▄▅▃▅█▆▆▆▃█▂▄▁▇▅▄▅▃▅
wandb: train_batches/train_loss ▆▅▃▆▂▆▆▅▅▅▅▅▅▃▆▅▅▁▆▆▆▁▅▅█▃▆▇▅▆▃▆▅▃▅▅▆▆▃▇
wandb:       train_epochs/epoch ▁
wandb:   train_epochs/train_acc ▁
wandb:  train_epochs/train_loss ▁
wandb:        val_batches/batch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:      val_batches/val_acc ▂▄▅▃▃▃▃▃▃▃▃█▄▅▄▃▂▄▆▂▁▃▂▄▃▃▄▂▃▄▄▅▄▃▄▄▅▃▄▆
wandb:     val_batches/val_loss ▆█▁▅▅▂▄▅▂█▆▇▅▅▇▆██▇▆▄▅█▆▇▂▅▂▆▂▅▅▃▆▇▄▄▆▂▅
wandb:         val_epochs/epoch ▁
wandb:       val_epochs/val_acc ▁
wandb:      val_epochs/val_loss ▁
wandb: 
wandb: Run summary:
wandb:      train_batches/batch 14200
wandb:  train_batches/train_acc 0.04814
wandb: train_batches/train_loss 4.73017
wandb:       train_epochs/epoch 0
wandb:   train_epochs/train_acc 0.01756
wandb:  train_epochs/train_loss 4.47712
wandb:        val_batches/batch 2500
wandb:      val_batches/val_acc 0.07359
wandb:     val_batches/val_loss 4.46668
wandb:         val_epochs/epoch 0
wandb:       val_epochs/val_acc 0.02113
wandb:      val_epochs/val_loss 4.47454
wandb: 
wandb: 🚀 View run solar-sponge-64 at: https://wandb.ai/tfg_oriol/action_classification/runs/0k0t20or
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230509_110307-0k0t20or/logs
val Loss: 4.4745 Acc: 0.0211
Training complete in 368m 35s
Best val Acc: 0.021130
Model saved at /home-net/omartinez/TFG/weights/vanilla_attention_7

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd8aadb-3277-4fc1-8d5d-08260a3e5e66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, reduce\n",
    "from torch import Tensor\n",
    "from torch import einsum\n",
    "from typing import Tuple\n",
    "from omegaconf import DictConfig\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4b4e6e-3be1-4625-aafd-934fc9ad9444",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(r'C:\\Users\\34609\\VisualStudio\\TFG\\attention_zoo')  \n",
    "from attentions.abstract_attention import AbstractAttention\n",
    "sys.path.append(r'C:\\Users\\34609\\VisualStudio\\TFG')  \n",
    "from utils import iterative_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "131b664b-da41-42ba-9179-b22cb34bab66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function iterative_inv at 0x000002705443B5B0>\n"
     ]
    }
   ],
   "source": [
    "print(iterative_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f926a57a-4114-4a73-87b2-d9ddb4cec8b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg = omegaconf.OmegaConf.create({\n",
    "    'model': {\n",
    "        'model': {\n",
    "            'ATTENTION': 'nystromformer',\n",
    "            'eps': 1e-8,\n",
    "            'num_landmarks': 64,\n",
    "            'pinv_iterations': 64,\n",
    "            'NUM_CLASSES': 96,\n",
    "            'PATCH_SIZE': 16,\n",
    "            'DEPTH': 2,\n",
    "            'HEADS': 4\n",
    "        },\n",
    "        'ATTENTION': 'nystromformer',\n",
    "        'eps': 1e-8,\n",
    "        'num_landmarks': 64,\n",
    "        'pinv_iterations': 64,\n",
    "        'NUM_CLASSES': 96,\n",
    "        'PATCH_SIZE': 16,\n",
    "        'DEPTH': 2,\n",
    "        'HEADS': 4\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fc71ad6-a37e-4864-9bfe-e023e856e44a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NystromformerAttention(AbstractAttention):\n",
    "    def __init__(self, hpars: DictConfig, n: int, h: int, in_feat: int, out_feat: int) -> None:\n",
    "        super().__init__(n=n, h=h, in_feat=in_feat, out_feat=out_feat)\n",
    "        self.model_params = hpars.model\n",
    "\n",
    "        self.n_orig = None\n",
    "        self.eps = self.model_params.eps\n",
    "        self.num_landmarks = self.model_params.num_landmarks\n",
    "        self.pinv_iterations = self.model_params.pinv_iterations\n",
    "\n",
    "    # This is an optional function that if overwritten, it adds the necessary padding\n",
    "    def pad_input(self, x: Tensor) -> Tensor:\n",
    "        b, n, d, f, m = *x.shape, self.num_landmarks\n",
    "        remainder = d % m\n",
    "\n",
    "        self.original_dim = d\n",
    "        if remainder > 0:\n",
    "            padding = m - (d % m)\n",
    "            x = F.pad(x, (0, 0, padding, 0), value=0)\n",
    "\n",
    "        self.n_orig = d\n",
    "\n",
    "        return x\n",
    "\n",
    "    def apply_attention(self, Q: Tensor, K: Tensor, V: Tensor, debug: bool = False, mask=None) -> Tuple[Tensor, Tensor]:\n",
    "        b, h, n, d_head, m, iters, eps = *Q.shape, self.num_landmarks, self.pinv_iterations, self.eps\n",
    "\n",
    "        # If necessary, add padding to the embeddings to be divisible\n",
    "        Q,K,V = map(lambda t: self.pad_input(t), (Q, K, V))\n",
    "        \n",
    "        for mat in [Q, K, V]:\n",
    "            isnan = torch.isnan(mat).any()\n",
    "            print(f'Nans 1: {isnan}')\n",
    "\n",
    "        # set masked positions to 0 in queries, keys, values\n",
    "        if mask is not None:\n",
    "            mask = rearrange(mask, 'b n -> b () n')\n",
    "            Q, K, V = map(lambda t: t * mask[..., None], (Q, K, V))\n",
    "        Q *= (d_head ** -0.5)\n",
    "\n",
    "        # generate landmarks by sum reduction, and then calculate mean using the mask\n",
    "        l = ceil(n / m)\n",
    "        print(f'l: {l}')\n",
    "        landmark_einops_eq = '... (n l) d -> ... n d'\n",
    "        q_landmarks = reduce(Q, landmark_einops_eq, 'sum', l=l)\n",
    "        k_landmarks = reduce(K, landmark_einops_eq, 'sum', l=l)\n",
    "\n",
    "        # calculate landmark mask, and also get sum of non-masked elements in preparation for masked mean\n",
    "        divisor = l\n",
    "        if mask is not None:\n",
    "            mask_landmarks_sum = reduce(mask, '... (n l) -> ... n', 'sum', l=l)\n",
    "            divisor = mask_landmarks_sum[..., None] + eps\n",
    "            mask_landmarks = mask_landmarks_sum > 0\n",
    "\n",
    "        # masked mean (if mask exists)\n",
    "        q_landmarks /= divisor\n",
    "        k_landmarks /= divisor\n",
    "\n",
    "        # similarities\n",
    "        einops_eq = '... i d, ... j d -> ... i j'\n",
    "        sim1 = einsum(einops_eq, Q, k_landmarks)\n",
    "        sim2 = einsum(einops_eq, q_landmarks, k_landmarks)\n",
    "        sim3 = einsum(einops_eq, q_landmarks, K)\n",
    "\n",
    "        # masking\n",
    "        if mask is not None:\n",
    "            mask_value = -torch.finfo(Q.dtype).max\n",
    "            sim1.masked_fill_(~(mask[..., None] * mask_landmarks[..., None, :]), mask_value)\n",
    "            sim2.masked_fill_(~(mask_landmarks[..., None] * mask_landmarks[..., None, :]), mask_value)\n",
    "            sim3.masked_fill_(~(mask_landmarks[..., None] * mask[..., None, :]), mask_value)\n",
    "\n",
    "        # eq (15) in the paper and aggregate values\n",
    "        attn1, attn2, attn3 = map(lambda t: t.softmax(dim=-1), (sim1, sim2, sim3))\n",
    "        \n",
    "        for mat in [attn1, attn2, attn3]:\n",
    "            isnan = torch.isnan(mat).any()\n",
    "            print(f'Nans 2: {isnan}')\n",
    "        \n",
    "        attn2_inv = iterative_inv(attn2, iters)\n",
    "        isnan = torch.isnan(attn2_inv).any()\n",
    "        print(f'Nans 3: {isnan}')\n",
    "        \n",
    "        out = (attn1 @ attn2_inv) @ (attn3 @ V)\n",
    "\n",
    "        # Merge the multiple heads into one\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = out[:, -n:, :]  # Select only last n to get rid of the padded ones\n",
    "\n",
    "        return out, None if not debug else (attn1 @ attn2_inv @ attn3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd4502bb-941a-41fc-80f1-82c3b6498961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.rand(4, 9800, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c8cd537-f27c-4095-950a-247f60cdeefd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, cfg, dim, num_heads=4, num_patches=9800, proj_drop=0., attn_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = NystromformerAttention(cfg, in_feat=dim, out_feat=dim, n=num_patches, h=num_heads)\n",
    "        self.qkv = nn.Linear(dim, dim * 3)  # (B, N, C) -> (B, N, C * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        print(f'x shape; {x.shape}')\n",
    "        qkv = self.qkv(x)\n",
    "        print(f'qkv: {self.qkv(x).shape}')\n",
    "        qkv = rearrange(qkv, 'b n (c h1 c1) -> b n c h1 c1', h1=self.num_heads, c1=C//self.num_heads)\n",
    "        print(f'qkv reshaped: {qkv.shape}')\n",
    "        qkv = rearrange(qkv, 'b n c h1 c1 -> c b h1 n c1')\n",
    "        print(f'qkv reshaped and permuted: {qkv.shape}')\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        print(f'q: {q.shape}, k: {k.shape}, v: {v.shape}')\n",
    "        output = self.attention.apply_attention(Q=q, K=k, V=v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79260977-a27d-4236-8c8b-4a31201afe91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "att = MultiHeadAttention(cfg, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "148e1319-47eb-4076-8b22-64e98dd8bbf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape; torch.Size([4, 9800, 768])\n",
      "qkv: torch.Size([4, 9800, 2304])\n",
      "qkv reshaped: torch.Size([4, 9800, 3, 4, 192])\n",
      "qkv reshaped and permuted: torch.Size([3, 4, 4, 9800, 192])\n",
      "q: torch.Size([4, 4, 9800, 192]), k: torch.Size([4, 4, 9800, 192]), v: torch.Size([4, 4, 9800, 192])\n",
      "Nans 1: False\n",
      "Nans 1: False\n",
      "Nans 1: False\n",
      "l: 154\n",
      "Nans 2: False\n",
      "Nans 2: False\n",
      "Nans 2: False\n",
      "Nans 3: False\n"
     ]
    }
   ],
   "source": [
    "out = att.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5506dbc8-38f4-4d56-86e0-63d8aedfda7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

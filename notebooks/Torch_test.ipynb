{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff2d875-dee1-4827-b230-ad20ed61a4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, reduce, repeat\n",
    "import omegaconf\n",
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71002102-5279-49c2-80d7-cd506611c8db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.rand(4, 3, 200, 112, 112) # b, c, t, w, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc980c-14ee-49a9-aa6e-76e86e5c69b9",
   "metadata": {},
   "source": [
    "## PATCH TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d89f0b19-db21-45e2-a0a3-c5bc61ccd0fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=112, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = [img_size, img_size]\n",
    "        patch_size = [patch_size, patch_size]\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, T, H, W = x.shape\n",
    "        x = rearrange(x, 'b c t h w -> (b t) c h w')\n",
    "        print(f'x shape 1: {x.shape}')\n",
    "        x = self.proj(x)\n",
    "        print(f'x shape 2: {x.shape}')\n",
    "        W = x.size(-1)\n",
    "        # x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = rearrange(x, '(b t) c w h -> b (w h t) c', t=200) \n",
    "        return x, T, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e262f89-ed57-44b4-bc22-da65e3d8ff48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patching = PatchEmbed(img_size=112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ed2b3f3-a1fc-4439-916d-339ec2e2c5e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_patches = x.shape[1]\n",
    "num_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baddf7ea-f800-4e8f-8f5d-f21c1e089a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape 1: torch.Size([40, 3, 112, 112])\n",
      "x shape 2: torch.Size([40, 768, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "x, T, W = patching.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a532369e-4655-4b6f-b685-c5816d7d16c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([4, 490, 768])\n",
      "T: 10\n",
      "W: 7\n"
     ]
    }
   ],
   "source": [
    "print(f'x shape: {x.size()}') # ( frames x batches ), nÂº patches, patch_embed = (3 x 16 x 16)\n",
    "print(f'T: {T}')\n",
    "print(f'W: {W}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e63df-850b-4953-8d8f-85e8cf8d8da7",
   "metadata": {},
   "source": [
    "## MULTIHEAD ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6fc7db1-5558-4a2e-b96b-06f8678abda0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(r'C:\\Users\\34609\\VisualStudio\\TFG\\attention_zoo')  \n",
    "from base_attention import BaseAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cafefb9-cf55-48fc-9e79-d8595cf34931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_V1.YAML\n",
    "# cfg = omegaconf.OmegaConf.create({\n",
    "#     'model': {\n",
    "#         'ATTENTION' : 'rela_attention'\n",
    "#     }\n",
    "# })\n",
    "\n",
    "# MODEL_V2.YAML\n",
    "# cfg = omegaconf.OmegaConf.create({\n",
    "#     'model': {\n",
    "#         'ATTENTION' : 'skyformer',\n",
    "#         'accumulation': 1,\n",
    "#         'num_feats': 128\n",
    "#     }\n",
    "# })\n",
    "\n",
    "# MODEL_V3.YAML\n",
    "# cfg = omegaconf.OmegaConf.create({\n",
    "#     'model': {\n",
    "#         # 'ATTENTION': 'nystromformer',\n",
    "#         'ATTENTION': 'cosformer',\n",
    "#         'eps': 1e-8,\n",
    "#         'num_landmarks': 64,\n",
    "#         'pinv_iterations': 64\n",
    "#     }\n",
    "# })\n",
    "\n",
    "# MODEL_V4.YAML\n",
    "cfg = omegaconf.OmegaConf.create({\n",
    "    'model': {\n",
    "        'model': {\n",
    "            'ATTENTION' : 'fastformer',\n",
    "            'use_rotary_emb': False\n",
    "        },\n",
    "    'ATTENTION': 'fastformer'\n",
    "    }\n",
    "})\n",
    "\n",
    "# MODEL_V5.YAML\n",
    "# cfg = omegaconf.OmegaConf.create({\n",
    "#     'model': {\n",
    "#         'ATTENTION' : 'linformer',\n",
    "#         'proj_feats': 64 \n",
    "#     }\n",
    "# })\n",
    "\n",
    "# MODEL_V6.YAML\n",
    "# cfg = omegaconf.OmegaConf.create({\n",
    "#     'model': {\n",
    "#         'ATTENTION' : 'performer',\n",
    "#         'kernel_type': 'relu'\n",
    "#     }\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8956b7b-d2ba-4e75-a403-28350ff64f13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, cfg, dim, num_heads=4, num_patches=num_patches, proj_drop=0., attn_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = BaseAttention.init_att_module(cfg, in_feat=dim, out_feat=dim, n=num_patches, h=num_heads)\n",
    "        self.qkv = nn.Linear(dim, dim * 3)  # (B, N, C) -> (B, N, C * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        print(f'x shape; {x.shape}')\n",
    "        qkv = self.qkv(x)\n",
    "        print(f'qkv: {self.qkv(x).shape}')\n",
    "        qkv = rearrange(qkv, 'b n (c h1 c1) -> b n c h1 c1', h1=self.num_heads, c1=C//self.num_heads)\n",
    "        print(f'qkv reshaped: {qkv.shape}')\n",
    "        qkv = rearrange(qkv, 'b n c h1 c1 -> c b h1 n c1')\n",
    "        print(f'qkv reshaped and permuted: {qkv.shape}')\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        print(f'q: {q.shape}, k: {k.shape}, v: {v.shape}')\n",
    "        output = self.attention.apply_attention(Q=q, K=k, V=v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b2ffe6d-ada9-4ad4-a827-c13acd58c74f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(cfg=cfg, dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9546c9f4-449b-4d3f-89fc-bbc496a435fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = mha.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13335908-9114-42b5-b8be-2b20655c907c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 9800, 768])\n",
      "Scores: tensor([[[[-3.5504e-02,  2.3289e-03, -5.8681e-04,  8.2961e-03,  6.9169e-03,\n",
      "           -1.4703e-02, -8.4640e-03, -1.2492e-02,  5.7824e-03,  3.8530e-03,\n",
      "           -1.0258e-01,  1.6425e-02,  1.3531e-02,  3.9351e-02,  1.3068e-03,\n",
      "            5.4774e-03,  1.1961e-01,  1.9300e-03, -1.6626e-04,  2.6109e-04,\n",
      "            1.7511e-02, -1.5955e-01, -2.9087e-02, -7.9597e-03, -3.8975e-03,\n",
      "           -2.4036e-03,  9.2822e-02, -2.4657e-03,  1.3981e-02, -3.3475e-02,\n",
      "            4.6561e-03,  8.6578e-03,  9.1577e-03, -7.9863e-04,  2.3493e-02,\n",
      "            3.2347e-02,  2.6546e-04, -6.5840e-05, -8.7299e-03, -2.2609e-02,\n",
      "           -1.2629e-03,  2.0892e-02, -3.7772e-02,  2.2706e-03, -3.6381e-02,\n",
      "            5.4902e-03,  3.8956e-03,  9.6150e-02, -3.9908e-03,  2.9058e-02,\n",
      "           -1.8976e-02, -3.3231e-03,  9.0794e-03, -1.2563e-02, -3.8308e-03,\n",
      "            3.3314e-03,  2.7973e-02,  2.4643e-03, -4.9220e-03,  2.3310e-02,\n",
      "           -1.6724e-03,  1.2164e-02,  7.2768e-03,  6.7981e-04, -1.3762e-02,\n",
      "           -3.4693e-03,  4.3697e-03,  6.7657e-03,  1.8584e-02,  3.8682e-04,\n",
      "            2.1250e-02, -1.8281e-02,  1.1592e-02,  1.4737e-03,  6.6195e-03,\n",
      "            4.7474e-03, -3.8970e-04, -9.7981e-04,  5.7871e-02,  1.3884e-02,\n",
      "           -2.0787e-02,  2.2136e-04, -4.8479e-03, -1.3324e-02,  1.0146e-02,\n",
      "            3.1435e-03,  2.8280e-03, -1.2732e-03, -1.0366e-02, -1.9727e-02,\n",
      "           -4.2593e-03,  6.2481e-03, -1.6320e-02,  1.3675e-02, -3.6609e-04,\n",
      "           -6.4929e-04,  8.1053e-03, -2.6545e-02,  1.3379e-02,  4.2806e-02,\n",
      "            5.4763e-02,  3.2719e-03, -2.6842e-03,  7.8719e-05, -1.9572e-05,\n",
      "            7.1995e-03, -4.8275e-02,  3.2954e-03,  5.9890e-02, -5.0720e-03,\n",
      "            5.2402e-04,  8.1303e-03,  1.0245e-02,  7.1493e-03,  1.7740e-02,\n",
      "            2.5715e-03, -1.7350e-02, -4.8066e-02,  1.6539e-02, -4.6994e-03,\n",
      "           -1.5158e-02, -6.9285e-03,  5.3937e-03, -3.8139e-03,  5.9189e-04,\n",
      "           -8.7876e-03,  2.3272e-03,  4.0260e-03, -1.3789e-03,  2.2499e-02,\n",
      "           -1.1976e-02,  1.0059e-02,  4.2747e-02,  5.5004e-02, -1.2173e-02,\n",
      "           -2.4227e-02,  8.0507e-03, -6.3121e-04,  1.6304e-02, -2.7631e-05,\n",
      "            3.9795e-03,  6.7607e-02, -1.1405e-02, -9.7671e-03,  6.9013e-03,\n",
      "            3.9542e-02,  2.8462e-02, -4.0850e-02,  5.6150e-03, -8.5865e-03,\n",
      "            3.3098e-03, -5.7892e-03, -7.4372e-05, -1.3556e-02, -5.8205e-03,\n",
      "           -3.1586e-03,  3.9912e-02, -9.8118e-03, -3.1465e-03,  1.7337e-02,\n",
      "            9.0484e-03,  8.8259e-03,  1.0474e-02,  9.4658e-03,  4.0392e-05,\n",
      "           -2.2340e-02, -1.1580e-04, -3.6666e-02,  9.2156e-02, -2.5960e-03,\n",
      "            2.7804e-03, -5.3557e-04, -1.0151e-03,  1.6932e-02, -2.6569e-02,\n",
      "            6.8432e-03, -8.7959e-02,  7.5158e-03, -2.8902e-05, -5.0652e-03,\n",
      "           -3.3118e-02,  3.2875e-02,  4.5724e-03,  3.9904e-03, -6.9936e-03,\n",
      "           -7.9766e-02,  3.8588e-02, -1.1732e-02,  1.9896e-02,  1.7505e-02,\n",
      "           -2.2653e-04, -2.0044e-02]],\n",
      "\n",
      "         [[ 3.6201e-03,  1.6687e-03,  5.9906e-03,  6.5437e-02,  4.8537e-03,\n",
      "           -2.9029e-02, -8.3227e-03,  8.3441e-04, -1.0320e-02,  5.5419e-03,\n",
      "           -3.5276e-02, -6.1278e-03,  3.0388e-04, -1.7857e-02,  1.7632e-02,\n",
      "            2.2686e-04,  1.3171e-03, -9.9233e-04, -5.3958e-03, -1.9138e-02,\n",
      "           -1.2269e-02,  2.3355e-02, -6.2071e-03,  1.1565e-03, -2.2975e-03,\n",
      "            1.4029e-03,  1.5525e-02,  8.9167e-03,  1.2699e-03, -4.4965e-02,\n",
      "           -2.5334e-02,  2.0291e-02, -2.4931e-02,  3.0068e-02,  4.4055e-03,\n",
      "            1.7168e-02,  2.6597e-02, -1.4725e-02,  8.5685e-02, -3.5273e-02,\n",
      "           -6.1237e-04, -4.4625e-03, -1.3325e-03, -9.3162e-02,  9.8603e-03,\n",
      "            8.0163e-03, -1.7332e-02, -9.5123e-03,  5.5722e-03, -2.5162e-03,\n",
      "           -5.5693e-05,  3.4486e-03,  5.5899e-03,  1.2630e-02,  6.3509e-02,\n",
      "            1.8021e-02,  1.1706e-02, -7.5013e-04, -2.0201e-02,  2.1954e-02,\n",
      "            1.1041e-02, -4.0278e-04, -8.2417e-03,  1.7286e-03,  3.4916e-02,\n",
      "            1.9791e-03,  3.6955e-03,  1.4490e-04, -2.3566e-02, -5.5514e-03,\n",
      "           -2.2444e-02,  4.9246e-02, -9.4677e-03, -4.9793e-02,  3.8069e-03,\n",
      "           -1.2082e-02, -1.0801e-02,  5.4389e-02, -8.1852e-04, -7.6414e-03,\n",
      "            2.8343e-03,  1.4101e-02,  1.3051e-03, -2.0387e-04,  7.2309e-03,\n",
      "            2.1146e-02,  1.0694e-02,  1.0207e-03, -2.3888e-02,  2.4757e-02,\n",
      "           -6.1768e-03, -1.0525e-03, -1.9355e-03, -1.0310e-02, -5.4718e-03,\n",
      "            4.9194e-02, -6.4521e-03, -1.3676e-02, -2.1169e-02, -9.2977e-03,\n",
      "            4.1903e-03, -1.2718e-02,  1.7429e-02,  3.8760e-02,  6.2114e-03,\n",
      "           -1.1181e-03,  5.2120e-03,  3.0490e-03,  3.6042e-02, -6.0142e-02,\n",
      "           -3.4557e-03,  2.6317e-02, -6.1551e-02, -4.6383e-02,  1.1681e-02,\n",
      "           -3.6847e-03,  6.2096e-03, -5.2104e-03,  1.1440e-03,  1.6214e-03,\n",
      "           -2.0382e-03, -2.2516e-02, -2.8651e-02, -1.9279e-03,  1.5684e-03,\n",
      "            1.1013e-02,  6.5963e-02,  1.1271e-02, -2.6397e-02,  1.4122e-02,\n",
      "            1.5211e-04, -4.4685e-03,  5.3137e-03, -4.0527e-03, -3.7566e-02,\n",
      "            2.6027e-02, -1.3481e-02, -2.1267e-02, -2.5762e-02,  9.2260e-03,\n",
      "            3.9188e-02, -6.7474e-03, -3.5256e-05,  1.3409e-02,  3.8711e-03,\n",
      "            1.7698e-04, -6.9776e-03, -5.1035e-03, -1.0180e-01,  1.4025e-02,\n",
      "           -7.0262e-02, -4.1326e-02,  3.3737e-03, -2.1987e-02, -4.4850e-03,\n",
      "           -4.6410e-03, -7.5707e-03, -5.3567e-02,  2.4600e-04, -1.5771e-03,\n",
      "            1.3083e-02, -1.9701e-03, -9.9748e-03, -7.0796e-04,  3.5739e-02,\n",
      "           -2.0888e-02, -3.8793e-03, -1.5846e-03,  1.5935e-02,  4.1649e-03,\n",
      "           -1.9298e-02, -6.7116e-04,  1.4234e-02, -4.8693e-02, -3.5565e-04,\n",
      "           -1.4135e-02,  9.3081e-03,  4.4123e-02,  2.3669e-02, -1.4590e-02,\n",
      "            3.0780e-02,  1.1213e-02,  1.8305e-02,  1.1197e-02, -4.0813e-02,\n",
      "           -3.2631e-03, -4.6175e-03,  1.7414e-02,  7.0924e-03,  5.2393e-03,\n",
      "           -5.5692e-03,  1.7457e-03]],\n",
      "\n",
      "         [[ 3.7124e-02, -6.4887e-03, -9.2450e-04, -1.5904e-02, -9.0466e-03,\n",
      "           -2.8940e-02, -3.2602e-03,  6.7517e-02,  7.6644e-04,  1.2757e-02,\n",
      "           -6.3016e-03,  1.1360e-02, -3.7212e-03,  7.2133e-02,  3.1487e-03,\n",
      "            1.5981e-02, -4.6423e-03, -1.6913e-02,  7.6140e-03, -2.5390e-02,\n",
      "            9.0974e-03, -3.9743e-02,  5.3580e-03, -7.1429e-04, -9.1197e-03,\n",
      "           -4.0647e-02,  1.5219e-03, -5.6719e-02, -1.8298e-04,  6.0835e-03,\n",
      "            7.0248e-02,  4.1989e-04,  2.4010e-03, -3.4162e-03,  5.0523e-04,\n",
      "           -1.5405e-02,  5.0209e-03, -1.4744e-03, -1.1016e-02, -7.2107e-03,\n",
      "            2.3715e-02, -2.4242e-03, -2.4786e-03, -6.2475e-03, -5.9543e-03,\n",
      "            2.0451e-02, -6.5677e-03, -7.5084e-03, -2.4734e-02, -3.7678e-02,\n",
      "            5.8312e-04,  6.3567e-03,  2.0941e-02, -1.9225e-02,  4.2720e-04,\n",
      "           -2.7487e-02,  4.3117e-03,  3.2550e-02,  1.8349e-04, -6.3013e-02,\n",
      "            2.5769e-02, -5.6435e-04, -1.8259e-02, -4.9411e-03, -7.2161e-03,\n",
      "           -7.0984e-03, -4.2490e-03,  1.9680e-02, -9.0094e-03, -7.1999e-03,\n",
      "            1.3933e-02,  2.9628e-03,  2.6668e-02,  4.7460e-02, -3.7077e-03,\n",
      "           -2.9620e-04, -5.6729e-03,  6.3202e-03, -4.1603e-02,  1.5202e-02,\n",
      "           -3.5459e-02,  5.5509e-02,  5.7641e-02,  1.0380e-02, -2.0806e-03,\n",
      "           -2.5626e-02, -2.4004e-02,  7.8742e-03, -6.1229e-03, -2.8665e-02,\n",
      "            7.0030e-03, -1.6755e-03, -7.5181e-03, -3.2685e-03, -5.3140e-02,\n",
      "           -5.0075e-03,  1.0096e-01,  7.0350e-04, -1.3242e-02,  3.0781e-02,\n",
      "           -8.0053e-02, -8.4448e-03, -5.8550e-04, -1.9311e-04,  1.3439e-02,\n",
      "            1.4095e-02,  2.7318e-03,  5.4467e-04,  2.7894e-02,  1.4638e-02,\n",
      "           -1.3606e-03,  1.5842e-02,  5.9673e-03, -1.2413e-02,  1.2300e-02,\n",
      "           -8.6804e-04, -2.1735e-03,  3.3801e-02, -3.1320e-03, -8.8063e-03,\n",
      "           -3.8320e-03,  1.6804e-02,  5.3859e-02, -2.1223e-03, -1.8327e-02,\n",
      "           -1.9613e-02,  3.6094e-04,  2.2530e-02,  7.6144e-03,  1.9684e-02,\n",
      "           -2.0658e-02, -1.8714e-03, -5.4991e-02, -9.2964e-03,  6.3719e-02,\n",
      "           -2.8380e-03,  2.2160e-03, -1.0637e-02, -2.0431e-02, -2.6380e-04,\n",
      "           -4.7334e-03,  7.0890e-03, -4.3457e-02, -1.9617e-02, -6.7890e-03,\n",
      "            2.0512e-03,  9.3852e-03, -6.7861e-03,  6.6483e-03, -1.1227e-03,\n",
      "           -9.4682e-02,  3.3095e-02, -8.0785e-05,  7.7777e-03,  1.9698e-02,\n",
      "           -2.9524e-02,  1.0434e-02, -1.6467e-02,  6.6359e-03, -1.0469e-02,\n",
      "            1.0371e-02, -2.2283e-03,  2.1676e-02,  1.5067e-02, -2.8383e-02,\n",
      "            1.1479e-02, -2.0561e-03, -1.1401e-02, -1.0457e-02, -1.8240e-03,\n",
      "           -2.4261e-03,  1.0384e-02, -1.1780e-04, -4.6891e-02, -1.4185e-03,\n",
      "           -1.9528e-03, -2.8210e-02,  8.1130e-04, -3.2656e-02,  5.9556e-03,\n",
      "           -3.5559e-02,  3.5715e-04, -3.4923e-03, -1.3087e-03, -1.4525e-03,\n",
      "           -6.0866e-03, -2.2797e-05, -3.3996e-03, -5.6863e-04,  3.8136e-03,\n",
      "            3.7833e-03,  8.8757e-03]],\n",
      "\n",
      "         [[ 3.3799e-02,  3.1185e-02, -7.4277e-03, -1.0531e-01, -1.2124e-01,\n",
      "            1.3278e-02, -6.8021e-02, -1.1953e-02,  2.8712e-03,  2.3056e-02,\n",
      "           -2.3573e-03, -1.9981e-03,  1.0031e-02, -1.3595e-02, -1.5799e-02,\n",
      "            1.2561e-02, -8.0014e-03, -2.2766e-02,  6.2528e-02, -3.9535e-03,\n",
      "           -1.2783e-02,  4.1565e-05,  4.2143e-02,  3.5190e-03,  1.7829e-04,\n",
      "           -2.7041e-02,  2.0113e-02,  1.3041e-03,  5.8961e-02, -2.2586e-02,\n",
      "            1.8801e-03,  3.1179e-04,  4.5152e-02,  1.6991e-03,  6.2501e-02,\n",
      "            1.5458e-03, -5.5694e-03, -7.0942e-03, -5.5020e-03, -7.0645e-02,\n",
      "            3.6812e-03, -1.2865e-03, -6.5210e-03, -1.5310e-02, -5.1046e-02,\n",
      "            2.8259e-03, -4.6339e-02, -6.2957e-02,  1.1943e-02,  2.6375e-02,\n",
      "            5.3266e-02,  2.5031e-03,  1.6590e-02,  3.0573e-02, -5.7208e-04,\n",
      "            5.3253e-03, -4.8272e-02, -3.8228e-03, -5.0678e-02, -2.4639e-03,\n",
      "           -6.2539e-02, -7.8096e-07, -2.0165e-02,  2.4340e-02, -1.4293e-03,\n",
      "            2.1165e-02, -1.3184e-02, -1.5449e-02, -1.2178e-01, -6.8101e-03,\n",
      "           -2.4735e-02, -2.1997e-02,  4.0188e-03,  3.2342e-02, -5.4465e-03,\n",
      "            4.1678e-05, -6.4720e-02,  2.6306e-03,  1.0832e-03,  1.8409e-04,\n",
      "           -3.0591e-04,  2.1522e-03,  1.5222e-02,  2.4108e-04,  6.0890e-02,\n",
      "           -3.4319e-03,  2.4616e-03, -2.6717e-02,  2.7457e-03,  8.1678e-03,\n",
      "            4.8321e-02,  1.0694e-02,  4.2339e-04,  1.0446e-02, -2.4992e-03,\n",
      "            1.9168e-02, -1.7456e-02, -1.2633e-03,  2.6136e-02,  1.0300e-02,\n",
      "            7.8700e-02, -1.5054e-02,  2.8026e-02,  2.9915e-02,  3.2009e-02,\n",
      "           -1.0560e-02, -5.8665e-03,  3.9076e-03,  2.2406e-03, -3.4175e-02,\n",
      "            5.6914e-02,  7.3255e-03,  1.0192e-02,  2.1023e-03,  1.5580e-02,\n",
      "           -1.2723e-03,  4.1280e-04,  2.6077e-02,  3.5211e-03,  3.4052e-02,\n",
      "            9.1992e-02,  1.3395e-03, -8.2467e-03, -7.9799e-03, -1.2330e-02,\n",
      "            1.0507e-02,  1.4501e-02,  2.0686e-04,  1.4502e-02,  1.9070e-04,\n",
      "            1.1479e-03,  5.9197e-03,  1.6271e-02,  2.9680e-02,  3.3418e-03,\n",
      "            3.8390e-02, -9.8991e-03, -2.1414e-03,  1.1695e-02,  1.2759e-02,\n",
      "           -7.5005e-03,  7.7618e-03, -6.4763e-03,  6.7998e-03, -3.0169e-02,\n",
      "           -2.5091e-02,  6.4059e-03,  2.9508e-03,  5.3061e-03,  1.4191e-03,\n",
      "           -2.7519e-04,  3.3017e-02,  6.0988e-03,  2.6115e-02,  1.4134e-03,\n",
      "           -8.9604e-03,  6.3933e-03, -2.5080e-02,  3.5121e-02,  8.3037e-03,\n",
      "            6.2360e-02, -2.1759e-02,  3.8893e-02,  8.2523e-03,  2.0863e-02,\n",
      "           -4.6667e-04, -1.0643e-02,  2.6861e-03,  1.1430e-03, -3.0572e-03,\n",
      "           -2.4665e-03, -1.3079e-02, -1.4596e-02,  6.2690e-03,  1.5199e-02,\n",
      "           -6.5982e-03, -9.5400e-03, -2.7969e-03,  2.6376e-02,  1.8830e-02,\n",
      "            7.1700e-05, -1.2844e-04, -1.7550e-02,  6.6798e-03, -4.3243e-03,\n",
      "            1.5053e-01,  1.3642e-02, -1.8053e-03, -1.4535e-03,  2.8334e-02,\n",
      "            6.0099e-03,  4.1773e-03]]]], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'Output shape: {out[0].shape}')\n",
    "# print(f'Scores: {out[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f0e25-7718-4ef8-a3b3-7686b4b9c74e",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8b1277f-8081-4f10-b42d-94a014133484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        print(f'in: {in_features} / hidden: {hidden_features} / out: {out_features}')\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca6e5350-d991-4a84-a428-9da47fcea60d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: 768 / hidden: 3072 / out: 768\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(in_features=768, hidden_features=4*768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2ac90ff-1f31-4636-b381-f3fa0026eb41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp_out = mlp.forward(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70b1022e-80b0-4173-a28e-647391a34da1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9800, 768])\n"
     ]
    }
   ],
   "source": [
    "print(mlp_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3530df4-e6a9-4ef6-8fcc-287738dbdc9b",
   "metadata": {},
   "source": [
    "## ATTENTION BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51136411-a8e9-4b2d-8797-7a9b1894c69a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, cfg, dim, num_heads, mlp_ratio=4., proj_drop=0., attn_drop=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = MultiHeadAttention(cfg, dim, num_heads, proj_drop, attn_drop)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(dim, mlp_hidden_dim, act_layer=act_layer, drop=proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83b859fb-acf1-4c5d-ae6d-bd0c3f45e9de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: 768 / hidden: 3072 / out: 768\n"
     ]
    }
   ],
   "source": [
    "block = Block(cfg, dim=768, num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c94f8c65-8279-46d1-a7a4-49be08248713",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape; torch.Size([1, 9800, 768])\n",
      "qkv: torch.Size([1, 9800, 2304])\n",
      "qkv reshaped: torch.Size([1, 9800, 3, 4, 192])\n",
      "qkv reshaped and permuted: torch.Size([3, 1, 4, 9800, 192])\n",
      "q: torch.Size([1, 4, 9800, 192]), k: torch.Size([1, 4, 9800, 192]), v: torch.Size([1, 4, 9800, 192])\n"
     ]
    }
   ],
   "source": [
    "block_out = block.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f10c68b-b50c-4449-924d-12f6f0a8aae7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9800, 768])\n"
     ]
    }
   ],
   "source": [
    "print(block_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f454a7-207c-4c7a-9503-33e7a2184c4e",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "444d8c94-8d62-4cf9-acbe-3838edb3f968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Model class with PatchTokenization + (MuliHeadAttention + MLP) x L + MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, img_size=112, patch_size=16, in_chans=3, embed_dim=768, num_classes=97, depth=2, num_heads=4, mlp_ratio=4.,\n",
    "                 proj_drop=0., attn_drop=0., norm_layer=nn.LayerNorm, num_frames=200, dropout=0., batch_size=1):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_frames = num_frames\n",
    "        self.patch_embed= PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches * self.num_frames\n",
    "        \n",
    "        # Positional Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(batch_size, num_patches+1, embed_dim))\n",
    "        # self.time_embed = nn.Parameter(torch.zeros(1, num_frames, embed_dim))\n",
    "                                       \n",
    "        # Attention Blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(cfg, embed_dim, num_heads, mlp_ratio, proj_drop, attn_drop, act_layer=nn.GELU, norm_layer=norm_layer)\n",
    "            for i in range(self.depth)])                            \n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        \n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, T, W = self.patch_embed(x)\n",
    "        \n",
    "        # add class token\n",
    "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1) # shape: (1, 1, embed) -> (batches, 1, embed)\n",
    "        print(f'cls_tokens shape: {cls_tokens.shape}')\n",
    "        x = torch.cat((cls_tokens, x), dim=1) # (batch, frames * patches, embed) -> (batch, frames * patches + 1, embed)\n",
    "        print(f'torch cat: {x.shape}')\n",
    "    \n",
    "        # add positional/temporal embedding\n",
    "        x = x + self.pos_embed\n",
    "        print(f'x + pos_embed: {x.shape}')\n",
    "    \n",
    "        for block in self.blocks:\n",
    "            x = block.forward(x)\n",
    "        # x = rearrange(x, 'b (p f) e -> b f p e', f=self.num_frames) # (batch x frames, patches, embed) -> (batch, frames, patch, embed)\n",
    "        # x = torch.mean(x, [1,2])\n",
    "        x = x[:, -1]\n",
    "        print(f'x shape: {x.shape}')\n",
    "        x = self.head(x)\n",
    "        return x               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89177871-3c91-4d01-8fb4-167545a9c903",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: 768 / hidden: 3072 / out: 768\n",
      "in: 768 / hidden: 3072 / out: 768\n"
     ]
    }
   ],
   "source": [
    "model = Model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c315da47-7c16-4946-af4e-614136b39afa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "params = model.named_parameters()\n",
    "count = 0\n",
    "for param in params:\n",
    "    # print(param[0])\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcd7861b-5755-4f9b-b32c-c2bd5f25314a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape 1: torch.Size([600, 3, 112, 112])\n",
      "x shape 2: torch.Size([600, 768, 7, 7])\n",
      "cls_tokens shape: torch.Size([3, 1, 768])\n",
      "torch cat: torch.Size([3, 9801, 768])\n",
      "x + pos_embed: torch.Size([3, 9801, 768])\n",
      "x shape; torch.Size([3, 9801, 768])\n",
      "qkv: torch.Size([3, 9801, 2304])\n",
      "qkv reshaped: torch.Size([3, 9801, 3, 4, 192])\n",
      "qkv reshaped and permuted: torch.Size([3, 3, 4, 9801, 192])\n",
      "q: torch.Size([3, 4, 9801, 192]), k: torch.Size([3, 4, 9801, 192]), v: torch.Size([3, 4, 9801, 192])\n",
      "x shape; torch.Size([3, 9801, 768])\n",
      "qkv: torch.Size([3, 9801, 2304])\n",
      "qkv reshaped: torch.Size([3, 9801, 3, 4, 192])\n",
      "qkv reshaped and permuted: torch.Size([3, 3, 4, 9801, 192])\n",
      "q: torch.Size([3, 4, 9801, 192]), k: torch.Size([3, 4, 9801, 192]), v: torch.Size([3, 4, 9801, 192])\n",
      "x shape: torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, 3, 200, 112, 112) # b, c, t, w, h\n",
    "model_out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "faf77c0e-be15-4623-9223-a2eb9d2e7d41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 97])\n"
     ]
    }
   ],
   "source": [
    "print(model_out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e4dc64-e881-4688-9d96-60d918f8319d",
   "metadata": {},
   "source": [
    "## Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb710f84-75cb-41f4-8c31-b5e2c2901e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import sin, cos, pow\n",
    "\n",
    "def pos_embed(\n",
    "    batch_size: int,\n",
    "    num_patches: int,\n",
    "    embed_dim: int\n",
    ") -> torch.tensor:\n",
    "    pos_embed = torch.zeros(num_patches, embed_dim)\n",
    "    \n",
    "    for i in range(num_patches):\n",
    "        for j in range(embed_dim):\n",
    "            if j % 2 == 0:\n",
    "                p = sin(i / pow(10000, ((2 * i) / embed_dim)))\n",
    "            else:\n",
    "                p = cos(i / pow(10000, ((2 * i) / embed_dim)))\n",
    "            pos_embed[i][j] = p\n",
    "    pos_embed = pos_embed.unsqueeze(0)\n",
    "    \n",
    "    return pos_embed        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "243093ce-8d40-42b4-9a7a-36e9c34cef9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 9801):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_length, embedding_dim]`` orignally [seq, batch]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4b3aff4-ed29-4920-bc3f-3891127f889f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "div_term = torch.exp(torch.arange(1, 768, 2) * (-math.log(10000.0) / 768))\n",
    "pe = torch.zeros(1, 9801, 768)\n",
    "position = torch.arange(9801).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c14477d-dc70-47ed-9bda-0a0a32ff6afe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9801, 768])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "pe.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27deae3b-3ba3-48b6-928d-46afee81af2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9801, 768])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(2, 9801, 768)\n",
    "a = x + pe\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e4a3444-5ab9-4525-9ee7-2786fdccc578",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9800, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbc86d97-acf9-4bd6-981c-469fd3c3bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token = nn.Parameter(torch.zeros(1, 1, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fbeedce-1e98-43c0-97e5-92bab31c56d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_tokens = cls_token.expand(x.size(0), -1, -1)\n",
    "cls_tokens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2907b41d-aba9-4ca0-a3bb-96a219938e3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.cat((cls_tokens, x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88a70dc3-3c80-4e8e-9beb-f52f6ed7e355",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9801, 768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_enc = PositionalEncoding(d_model=768, max_len=9801)\n",
    "x = pos_enc.forward(x)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e6859f-a8f3-4d29-a6ae-c60268bd8bc8",
   "metadata": {},
   "source": [
    "## RANDOM TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2a5e5df-e02b-4a2f-86c9-406208a13f15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n",
      "  ATTENTION: fastformer\n",
      "  use_rotary_emb: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(omegaconf.OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c2037c4-2caf-4b06-8013-123b10c901a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "a = Image.open('sample_image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "867867cb-36bb-4254-9d07-cd65ccc191f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d01285b0-478b-4445-b500-f477a1df0fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arr = np.asarray(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b437d246-15bf-47e7-ad4e-aca01b877d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0ca46d6-a2d8-4313-ad6c-2c371b3c8ea5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b = T.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "534424f2-4093-4271-bd47-b057f6c90524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = b(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ec4afa67-1585-42f6-b197-9b4d30b617a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 200, 150])\n"
     ]
    }
   ],
   "source": [
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8afa0f2-99a3-4702-95e1-2a8ff68b097f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-19 17:15:47.901014\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a5a4c6-0a4f-4ed6-9ed7-0edd2278dc81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff2d875-dee1-4827-b230-ad20ed61a4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, reduce, repeat\n",
    "import omegaconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71002102-5279-49c2-80d7-cd506611c8db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.rand(1, 3, 30, 240, 240) # b, c, t, w, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc980c-14ee-49a9-aa6e-76e86e5c69b9",
   "metadata": {},
   "source": [
    "## PATCH TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89f0b19-db21-45e2-a0a3-c5bc61ccd0fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=240, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = [img_size, img_size]\n",
    "        patch_size = [patch_size, patch_size]\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, T, H, W = x.shape\n",
    "        x = rearrange(x, 'b c t h w -> (b t) c h w')\n",
    "        print(f'x shape 1: {x.shape}')\n",
    "        x = self.proj(x)\n",
    "        print(f'x shape 2: {x.shape}')\n",
    "        W = x.size(-1)\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        return x, T, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e262f89-ed57-44b4-bc22-da65e3d8ff48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patching = PatchEmbed(img_size=480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baddf7ea-f800-4e8f-8f5d-f21c1e089a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape 1: torch.Size([30, 3, 240, 240])\n",
      "x shape 2: torch.Size([30, 768, 15, 15])\n"
     ]
    }
   ],
   "source": [
    "x, T, W = patching(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a532369e-4655-4b6f-b685-c5816d7d16c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([30, 225, 768])\n",
      "T: 30\n",
      "W: 15\n"
     ]
    }
   ],
   "source": [
    "print(f'x shape: {x.size()}') # ( frames x batches ), nÂº patches, patch_embed = (3 x 16 x 16)\n",
    "print(f'T: {T}')\n",
    "print(f'W: {W}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e63df-850b-4953-8d8f-85e8cf8d8da7",
   "metadata": {},
   "source": [
    "## MULTIHEAD ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6fc7db1-5558-4a2e-b96b-06f8678abda0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(r'C:\\Users\\34609\\VisualStudio\\TFG\\attention_zoo')  \n",
    "from base_attention import BaseAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cafefb9-cf55-48fc-9e79-d8595cf34931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg = omegaconf.OmegaConf.create({'name' : 'vanilla_attention'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8956b7b-d2ba-4e75-a403-28350ff64f13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, cfg, dim, num_heads=8, proj_drop=0., attn_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = BaseAttention.init_att_module(cfg, in_feat=dim, out_feat=dim, n=dim, h=dim)\n",
    "        self.qkv = nn.Linear(dim, dim * 3)  # (B, N, C) -> (B, N, C * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        print(f'qkv: {self.qkv(x).shape}')\n",
    "        qkv = rearrange(qkv, 'b n (c h w) -> b n c h w', h=self.num_heads, w=C//self.num_heads)\n",
    "        print(f'qkv reshaped: {qkv.shape}')\n",
    "        qkv = rearrange(qkv, 'b n c h w -> c b h n w')\n",
    "        print(f'qkv reshaped and permuted: {qkv.shape}')\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        output = self.attention.apply_attention(Q=q, K=k, V=v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b2ffe6d-ada9-4ad4-a827-c13acd58c74f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(cfg=cfg, dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9546c9f4-449b-4d3f-89fc-bbc496a435fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qkv: torch.Size([30, 225, 2304])\n",
      "qkv reshaped: torch.Size([30, 225, 3, 8, 96])\n",
      "qkv reshaped and permuted: torch.Size([3, 30, 8, 225, 96])\n"
     ]
    }
   ],
   "source": [
    "out = mha.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13335908-9114-42b5-b8be-2b20655c907c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([30, 225, 768])\n",
      "Scores: None\n"
     ]
    }
   ],
   "source": [
    "print(f'Output shape: {out[0].shape}')\n",
    "print(f'Scores: {out[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f0e25-7718-4ef8-a3b3-7686b4b9c74e",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8b1277f-8081-4f10-b42d-94a014133484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        print(f'in: {in_features} / hidden: {hidden_features} / out: {out_features}')\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca6e5350-d991-4a84-a428-9da47fcea60d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: 768 / hidden: 3072 / out: 768\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(in_features=768, hidden_features=4*768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2ac90ff-1f31-4636-b381-f3fa0026eb41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp_out = mlp.forward(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70b1022e-80b0-4173-a28e-647391a34da1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 225, 768])\n"
     ]
    }
   ],
   "source": [
    "print(mlp_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3530df4-e6a9-4ef6-8fcc-287738dbdc9b",
   "metadata": {},
   "source": [
    "## ATTENTION BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51136411-a8e9-4b2d-8797-7a9b1894c69a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, cfg, dim, num_heads, mlp_ratio=4., proj_drop=0., attn_drop=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = MultiHeadAttention(cfg, dim, num_heads, proj_drop, attn_drop)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(dim, mlp_hidden_dim, act_layer=act_layer, drop=proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83b859fb-acf1-4c5d-ae6d-bd0c3f45e9de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: 768 / hidden: 3072 / out: 768\n"
     ]
    }
   ],
   "source": [
    "block = Block(cfg, dim=768, num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c94f8c65-8279-46d1-a7a4-49be08248713",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qkv: torch.Size([30, 225, 2304])\n",
      "qkv reshaped: torch.Size([30, 225, 3, 4, 192])\n",
      "qkv reshaped and permuted: torch.Size([3, 30, 4, 225, 192])\n"
     ]
    }
   ],
   "source": [
    "block_out = block.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f10c68b-b50c-4449-924d-12f6f0a8aae7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 225, 768])\n"
     ]
    }
   ],
   "source": [
    "print(block_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f454a7-207c-4c7a-9503-33e7a2184c4e",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "444d8c94-8d62-4cf9-acbe-3838edb3f968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Model class with PatchTokenization + (MuliHeadAttention + MLP) x L + MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, img_size=240, patch_size=16, in_chans=3, embed_dim=768, num_classes=97, depth=2, num_heads=4, mlp_ratio=4.,\n",
    "                 proj_drop=0., attn_drop=0., norm_layer=nn.LayerNorm, num_frames=30, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.num_frames = num_frames\n",
    "        self.patch_embed= PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        # Positional Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(num_frames, num_patches+1, embed_dim))\n",
    "        # self.time_embed = nn.Parameter(torch.zeros(1, num_frames, embed_dim))\n",
    "                                       \n",
    "        # Attention Blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(cfg, embed_dim, num_heads, mlp_ratio, proj_drop, attn_drop, act_layer=nn.GELU, norm_layer=norm_layer)\n",
    "            for i in range(self.depth)])                            \n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        \n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, T, W = self.patch_embed(x)\n",
    "        \n",
    "        # add class token\n",
    "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1) # (1, 1, embed) -> (30, 1, embed)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) # (batch x frames, patches, embed) -> (batch x frames, patches + 1, embed)\n",
    "    \n",
    "        # add positional/temporal embedding\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block.forward(x)\n",
    "        x = rearrange(x, '(b f) p e -> b f p e', f=self.num_frames) # (batch x frames, patches, embed) -> (batch, frames, patch, embed)\n",
    "        x = torch.mean(x, [1,2])\n",
    "        x = self.head(x)\n",
    "        return x               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "89177871-3c91-4d01-8fb4-167545a9c903",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: 768 / hidden: 3072 / out: 768\n",
      "in: 768 / hidden: 3072 / out: 768\n"
     ]
    }
   ],
   "source": [
    "model = Model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bcd7861b-5755-4f9b-b32c-c2bd5f25314a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape 1: torch.Size([30, 3, 240, 240])\n",
      "x shape 2: torch.Size([30, 768, 15, 15])\n",
      "qkv: torch.Size([30, 226, 2304])\n",
      "qkv reshaped: torch.Size([30, 226, 3, 4, 192])\n",
      "qkv reshaped and permuted: torch.Size([3, 30, 4, 226, 192])\n",
      "qkv: torch.Size([30, 226, 2304])\n",
      "qkv reshaped: torch.Size([30, 226, 3, 4, 192])\n",
      "qkv reshaped and permuted: torch.Size([3, 30, 4, 226, 192])\n"
     ]
    }
   ],
   "source": [
    "model_out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "faf77c0e-be15-4623-9223-a2eb9d2e7d41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 97])\n"
     ]
    }
   ],
   "source": [
    "print(model_out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5eb7af-7642-4257-975d-724f55164657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

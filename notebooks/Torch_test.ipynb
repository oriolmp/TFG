{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ff2d875-dee1-4827-b230-ad20ed61a4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, reduce, repeat\n",
    "import omegaconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71002102-5279-49c2-80d7-cd506611c8db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.rand(8, 3, 30, 480, 480) # b, c, t, w, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc980c-14ee-49a9-aa6e-76e86e5c69b9",
   "metadata": {},
   "source": [
    "## PATCH TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89f0b19-db21-45e2-a0a3-c5bc61ccd0fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=480, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = [img_size, img_size]\n",
    "        patch_size = [patch_size, patch_size]\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, T, H, W = x.shape\n",
    "        x = rearrange(x, 'b c t h w -> (b t) c h w')\n",
    "        print(f'x shape 1: {x.shape}')\n",
    "        x = self.proj(x)\n",
    "        print(f'x shape 2: {x.shape}')\n",
    "        W = x.size(-1)\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c') \n",
    "        return x, T, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e262f89-ed57-44b4-bc22-da65e3d8ff48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patching = PatchEmbed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baddf7ea-f800-4e8f-8f5d-f21c1e089a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape 1: torch.Size([240, 3, 480, 480])\n",
      "x shape 2: torch.Size([240, 768, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "x, T, W = patching(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a532369e-4655-4b6f-b685-c5816d7d16c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([240, 900, 768])\n",
      "T: 30\n",
      "W: 30\n"
     ]
    }
   ],
   "source": [
    "print(f'x shape: {x.shape}')\n",
    "print(f'T: {T}')\n",
    "print(f'W: {W}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e63df-850b-4953-8d8f-85e8cf8d8da7",
   "metadata": {},
   "source": [
    "## MULTIHEAD ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6fc7db1-5558-4a2e-b96b-06f8678abda0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(r'C:\\Users\\34609\\VisualStudio\\TFG\\attention_zoo')  \n",
    "from base_attention import BaseAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cafefb9-cf55-48fc-9e79-d8595cf34931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg = omegaconf.OmegaConf.create({'name' : 'vanilla_attention'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8956b7b-d2ba-4e75-a403-28350ff64f13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, cfg, dim, num_heads=8, proj_drop=0., attn_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = BaseAttention.init_att_module(cfg, in_feat=dim, out_feat=dim, n=dim, h=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b2ffe6d-ada9-4ad4-a827-c13acd58c74f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'attention_zoo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mha \u001b[38;5;241m=\u001b[39m \u001b[43mMultiHeadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m, in \u001b[0;36mMultiHeadAttention.__init__\u001b[1;34m(self, cfg, dim, num_heads, proj_drop, attn_drop)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m=\u001b[39m num_heads\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention \u001b[38;5;241m=\u001b[39m \u001b[43mBaseAttention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_att_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_feat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_feat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\VisualStudio\\TFG\\attention_zoo\\base_attention.py:14\u001b[0m, in \u001b[0;36mBaseAttention.init_att_module\u001b[1;34m(att_config, n, h, in_feat, out_feat, debug, **kwargs)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# This method takes as input a name of an attention mechanism, and if implemented,\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# returns an instance of the corresponding object.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m att_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvanilla_attention\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mattention_zoo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattentions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvanilla_attention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvanilla_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VanillaAttention\n\u001b[0;32m     15\u001b[0m     att_mech \u001b[38;5;241m=\u001b[39m VanillaAttention(model_config, n, h, in_feat, out_feat)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m att_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrela_attention\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'attention_zoo'"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadAttention(cfg=cfg, dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9546c9f4-449b-4d3f-89fc-bbc496a435fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
